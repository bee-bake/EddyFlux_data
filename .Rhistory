filter(date == "flux_daily_count_NEE$date") %>%
select(date, DateTime, NEE_uStar_orig)%>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(frequency = n())%>%
ungroup() %>%
summarise(co2flux_umolm2s_mean = mean(NEE_uStar_orig))
daily_NEE <- flux_count_NEE  %>%
filter(date == flux_daily_count_NEE$date) %>%
select(date, DateTime, NEE_uStar_orig)%>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(frequency = n())%>%
ungroup() %>%
summarise(co2flux_umolm2s_mean = mean(NEE_uStar_orig))
daily_NEE <- flux_count_NEE  %>%
select(date, DateTime, NEE_uStar_orig)%>%
filter(date == flux_daily_count_NEE$date) %>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(frequency = n())%>%
ungroup() %>%
summarise(co2flux_umolm2s_mean = mean(NEE_uStar_orig))
daily_NEE <- flux_count_NEE  %>%
select(date, DateTime, NEE_uStar_orig)%>%
filter(date = flux_daily_count_NEE$date) %>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(frequency = n())%>%
ungroup() %>%
summarise(co2flux_umolm2s_mean = mean(NEE_uStar_orig))
daily_NEE <- flux_count_NEE  %>%
select(date, DateTime, NEE_uStar_orig)%>%
filter(date == flux_daily_count_NEE$date) %>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(co2flux_umolm2s_mean = mean(NEE_uStar_orig))
View(daily_NEE)
#Count the number of half hourly values and apply the cut-off
#For CO2
flux_daily_count_NEE <- flux_count_NEE %>%
group_by(date) %>%
summarise(frequency = n()) %>%
filter(frequency >= 24) %>% #count hh values and filter >=24 only
ungroup()
daily_NEE <- flux_count_NEE  %>%
select(date, DateTime, NEE_uStar_orig)%>%
which(date %in% flux_daily_count_NEE$date) %>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(co2flux_umolm2s_mean = mean(NEE_uStar_orig))
flux_count_NEE[flux_count_NEE$date %in% flux_daily_count_NEE$date, ]
daily_NEE <- flux_count_NEE  %>%
select(date, DateTime, NEE_uStar_orig)%>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(co2flux_umolm2s_mean = mean(NEE_uStar_orig))
View(daily_NEE)
#Select the dates
flux_count_NEE <- flux_count_NEE[flux_count_NEE$date %in% flux_daily_count_NEE$date, ]
daily_NEE <- flux_count_NEE  %>%
select(date, DateTime, NEE_uStar_orig)%>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(co2flux_umolm2s_mean = mean(NEE_uStar_orig))
View(daily_NEE)
#Format as per FLARE
targets_df_ustarCO2 <- daily_NEE %>%
drop_na(date)%>% # drop when we have timezone issues with daylight savings
mutate(datetime=(paste0(date," ","00:00:00")))%>%
mutate(Reservoir='fcre')%>% # change the name to the the reservoir code for FLARE
mutate(Depth_m = NA)%>%
select(-date, frequency)%>%
rename(site_id=Reservoir, # rename the columns for standard notation
depth=Depth_m)%>%
pivot_longer(cols=c(co2flux_umolm2s_mean), # make the wide data frame into a long one so each observation has a depth
names_to='variable',
values_to='observation')%>%
select(c('datetime', 'site_id', 'depth', "observation", 'variable')) # rearrange order of columns
#Format as per FLARE
targets_df_ustarCO2 <- daily_NEE %>%
drop_na(date)%>% # drop when we have timezone issues with daylight savings
mutate(datetime=(paste0(date," ","00:00:00")))%>%
mutate(Reservoir='fcre')%>% # change the name to the the reservoir code for FLARE
mutate(Depth_m = NA)%>%
select(-date)%>%
rename(site_id=Reservoir, # rename the columns for standard notation
depth=Depth_m)%>%
pivot_longer(cols=c(co2flux_umolm2s_mean), # make the wide data frame into a long one so each observation has a depth
names_to='variable',
values_to='observation')%>%
select(c('datetime', 'site_id', 'depth', "observation", 'variable')) # rearrange order of columns
View(targets_df_ustarCO2)
View(flux_count_NEE)
#Count the number of half hourly values and apply the cut-off
#For CO2
flux_daily_count_NEE <- flux_count_NEE %>%
group_by(date) %>%
summarise(frequency = n()) %>%
filter(frequency >= 1) %>% #count hh values and filter >=24 only
ungroup()
#Select the dates
flux_count_NEE <- flux_count_NEE[flux_count_NEE$date %in% flux_daily_count_NEE$date, ]
daily_NEE <- flux_count_NEE  %>%
select(date, DateTime, NEE_uStar_orig)%>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(co2flux_umolm2s_mean = mean(NEE_uStar_orig))
#Remove the NAs separately for columns NEE and CH4
flux_count_NEE <- u_star_applied %>%
select(date, DateTime, NEE_uStar_orig)%>%
na.omit()
flux_count_CH4 <- u_star_applied %>%
select(date, DateTime, ch4_flux_uStar_orig)%>%
na.omit()
#Count the number of half hourly values and apply the cut-off
#For CO2
flux_daily_count_NEE <- flux_count_NEE %>%
group_by(date) %>%
summarise(frequency = n()) %>%
filter(frequency >= 1) %>% #count hh values and filter >=24 only
ungroup()
#Select the dates
flux_count_NEE <- flux_count_NEE[flux_count_NEE$date %in% flux_daily_count_NEE$date, ]
daily_NEE <- flux_count_NEE  %>%
select(date, DateTime, NEE_uStar_orig)%>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(co2flux_umolm2s_mean = mean(NEE_uStar_orig))
#Remove the NAs separately for columns NEE and CH4
flux_count_NEE <- u_star_applied %>%
select(date, DateTime, NEE_uStar_orig)%>%
na.omit()
flux_count_CH4 <- u_star_applied %>%
select(date, DateTime, ch4_flux_uStar_orig)%>%
na.omit()
#Count the number of half hourly values and apply the cut-off
#For CO2
flux_daily_count_NEE <- flux_count_NEE %>%
group_by(date) %>%
summarise(frequency = n()) %>%
filter(frequency >= 24) %>% #count hh values and filter >=24 only
ungroup()
#Select the dates
flux_count_NEE <- flux_count_NEE[flux_count_NEE$date %in% flux_daily_count_NEE$date, ]
daily_NEE <- flux_count_NEE  %>%
select(date, DateTime, NEE_uStar_orig)%>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(co2flux_umolm2s_mean = mean(NEE_uStar_orig))
135/1055
#For CH4
flux_daily_count_CH4 <- flux_count_CH4 %>%
group_by(date) %>%
summarise(frequency = n()) %>%
filter(frequency >= 24) %>% #count hh values and filter >=24 only
ungroup()
#Select the dates
flux_count_CH4 <- flux_count_CH4[flux_count_CH4$date %in% flux_daily_count_CH4$date, ]
daily_CH4 <- flux_count_CH4  %>%
select(date, DateTime, ch4_flux_uStar_orig)%>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(ch4flux_umolm2s_mean = mean(ch4_flux_uStar_orig))
#Format as per FLARE
targets_df_ustarCH4 <- daily_CH4 %>%
drop_na(date)%>% # drop when we have timezone issues with daylight savings
mutate(datetime=(paste0(date," ","00:00:00")))%>%
mutate(Reservoir='fcre')%>% # change the name to the the reservoir code for FLARE
mutate(Depth_m = NA)%>%
select(-date)%>%
rename(site_id=Reservoir, # rename the columns for standard notation
depth=Depth_m)%>%
pivot_longer(cols=c(ch4flux_umolm2s_mean), # make the wide data frame into a long one so each observation has a depth
names_to='variable',
values_to='observation')%>%
select(c('datetime', 'site_id', 'depth', "observation", 'variable')) # rearrange order of columns
View(daily_CH4)
#For CH4
flux_daily_count_CH4 <- flux_count_CH4 %>%
group_by(date) %>%
summarise(frequency = n()) %>%
filter(frequency >= 1) %>% #count hh values and filter >=24 only
ungroup()
#Select the dates
flux_count_CH4 <- flux_count_CH4[flux_count_CH4$date %in% flux_daily_count_CH4$date, ]
daily_CH4 <- flux_count_CH4  %>%
select(date, DateTime, ch4_flux_uStar_orig)%>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(ch4flux_umolm2s_mean = mean(ch4_flux_uStar_orig))
flux_count_CH4 <- u_star_applied %>%
select(date, DateTime, ch4_flux_uStar_orig)%>%
na.omit()
#For CH4
flux_daily_count_CH4 <- flux_count_CH4 %>%
group_by(date) %>%
summarise(frequency = n()) %>%
filter(frequency >= 1) %>% #count hh values and filter >=24 only
ungroup()
#Select the dates
flux_count_CH4 <- flux_count_CH4[flux_count_CH4$date %in% flux_daily_count_CH4$date, ]
daily_CH4 <- flux_count_CH4  %>%
select(date, DateTime, ch4_flux_uStar_orig)%>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(ch4flux_umolm2s_mean = mean(ch4_flux_uStar_orig))
99/1121
#Extract the date column and add to the dataset
u_star_applied$date <- date(u_star_applied$DateTime)
#Remove the NAs separately for columns NEE and CH4
flux_count_NEE <- u_star_applied %>%
select(date, DateTime, NEE_uStar_orig)%>%
na.omit()
flux_count_CH4 <- u_star_applied %>%
select(date, DateTime, ch4_flux_uStar_orig)%>%
na.omit()
#Count the number of half hourly values and apply the cut-off
#For CO2
flux_daily_count_NEE <- flux_count_NEE %>%
group_by(date) %>%
summarise(frequency = n()) %>%
filter(frequency >= 24) %>% #count hh values and filter >=24 only
ungroup()
#Select the dates
flux_count_NEE <- flux_count_NEE[flux_count_NEE$date %in% flux_daily_count_NEE$date, ]
daily_NEE <- flux_count_NEE  %>%
select(date, DateTime, NEE_uStar_orig)%>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(co2flux_umolm2s_mean = mean(NEE_uStar_orig))
#Format as per FLARE
targets_df_ustarCO2 <- daily_NEE %>%
drop_na(date)%>% # drop when we have timezone issues with daylight savings
mutate(datetime=(paste0(date," ","00:00:00")))%>%
mutate(Reservoir='fcre')%>% # change the name to the the reservoir code for FLARE
mutate(Depth_m = NA)%>%
select(-date)%>%
rename(site_id=Reservoir, # rename the columns for standard notation
depth=Depth_m)%>%
pivot_longer(cols=c(co2flux_umolm2s_mean), # make the wide data frame into a long one so each observation has a depth
names_to='variable',
values_to='observation')%>%
select(c('datetime', 'site_id', 'depth', "observation", 'variable')) # rearrange order of columns
#For CH4
flux_daily_count_CH4 <- flux_count_CH4 %>%
group_by(date) %>%
summarise(frequency = n()) %>%
filter(frequency >= 24) %>% #count hh values and filter >=24 only
ungroup()
#Select the dates
flux_count_CH4 <- flux_count_CH4[flux_count_CH4$date %in% flux_daily_count_CH4$date, ]
daily_CH4 <- flux_count_CH4  %>%
select(date, DateTime, ch4_flux_uStar_orig)%>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(ch4flux_umolm2s_mean = mean(ch4_flux_uStar_orig))
#Format as per FLARE
targets_df_ustarCH4 <- daily_CH4 %>%
drop_na(date)%>% # drop when we have timezone issues with daylight savings
mutate(datetime=(paste0(date," ","00:00:00")))%>%
mutate(Reservoir='fcre')%>% # change the name to the the reservoir code for FLARE
mutate(Depth_m = NA)%>%
select(-date)%>%
rename(site_id=Reservoir, # rename the columns for standard notation
depth=Depth_m)%>%
pivot_longer(cols=c(ch4flux_umolm2s_mean), # make the wide data frame into a long one so each observation has a depth
names_to='variable',
values_to='observation')%>%
select(c('datetime', 'site_id', 'depth', "observation", 'variable')) # rearrange order of columns
#Make the datasets wider
targets_df_ustarCO2_wider <- targets_df_ustarCO2 %>%
pivot_wider(names_from = variable, values_from = observation)
targets_df_ustarCH4_wider <- targets_df_ustarCH4 %>%
pivot_wider(names_from = variable, values_from = observation)
p1 <- ggplot(targets_df_ustarCO2_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = co2flux_umolm2s_mean), colour = "blue", alpha = 0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CO2 daily mean") + xlab("") + ggtitle("Wind, U star and cut-off all applied") +
ylim(-20, 20)
p2 <- ggplot(targets_df_ustarCH4_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = ch4flux_umolm2s_mean), colour = "red", alpha = 0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CH4 daily mean") + xlab("") + ggtitle("Wind, U star and cut-off all applied") +
ylim(-0.045, 0.052)
#Plot the time series with the daily mean values
targets_without_stable_mean_wider <- targets_without_stable_mean %>%
pivot_wider(names_from = variable, values_from = observation)
p3 <- ggplot(targets_without_stable_mean_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = co2flux_umolm2s_mean), colour = "blue", alpha=0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CO2 daily mean") + xlab("") + ggtitle("U star filtering and cut-off not applied") +
ylim(-20, 20)
p4 <- ggplot(targets_without_stable_mean_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = ch4flux_umolm2s_mean), colour = "red", alpha = 0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CH4 daily mean") + xlab("") + ggtitle("U star filtering and cut-off not applied") +
ylim(-0.045, 0.052)
#Plot the time series with the daily mean values
targets_with_stable_mean_wider <- targets_with_stable_mean %>%
pivot_wider(names_from = variable, values_from = observation)
p5 <- ggplot(targets_with_stable_mean_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = co2flux_umolm2s_mean), colour = "blue", alpha=0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CO2 daily mean") + xlab("") + ggtitle("No U star filtering but cut-off applied at 20") +
ylim(-20, 20)
p6 <- ggplot(targets_with_stable_mean_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = ch4flux_umolm2s_mean), colour = "red", alpha = 0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CH4 daily mean") + xlab("") + ggtitle("No U star filtering but cut-off applied at 20") +
ylim(-0.045, 0.052)
#Plot all graphs
p3/p5/p1
p4/p6/p2
target_IceCover_binary <- function(current_file, historic_file){
## read in current data file
# Github, Googlesheet, etc.
current_df <- readr::read_csv(current_file, show_col_types = F)|>
dplyr::filter(Site == 50) |>
dplyr::select(Reservoir, DateTime,
dplyr::starts_with('ThermistorTemp')) |>
tidyr::pivot_longer(cols = starts_with('ThermistorTemp'),
names_to = 'depth',
names_prefix = 'ThermistorTemp_C_',
values_to = 'observation') |>
dplyr::mutate(Reservoir = ifelse(Reservoir == 'FCR',
'fcre',
ifelse(Reservoir == 'BVR',
'bvre', NA)),
datetime = lubridate::as_datetime(paste0(format(DateTime, "%Y-%m-%d %H"), ":00:00"))) |>
dplyr::group_by(Reservoir, datetime, depth) |>
dplyr::summarise(observation = mean(observation, na.rm = T),
.groups = 'drop') |>
dplyr::rename(site_id = Reservoir)
# the depths used to assess will change depending on the current depth of FCR
depths_use <- current_df |>
dplyr::mutate(depth = ifelse(depth == "surface", 0.1, depth)) |>
na.omit() |>
dplyr::group_by(datetime) |>
dplyr::summarise(top = min(as.numeric(depth)),
bottom = max(as.numeric(depth))) |>
tidyr::pivot_longer(cols = top:bottom,
values_to = 'depth')
current_on_off <- current_df |>
dplyr::mutate(depth = as.numeric(ifelse(depth == "surface", 0.1, depth))) |>
dplyr::right_join(depths_use, by = c('datetime', 'depth')) |>
dplyr::select(-depth) |>
tidyr::pivot_wider(names_from = name,
values_from = observation) |>
# Ice defined as when the top is cooler than the bottom, and temp below 4 oC
dplyr::mutate(temp_diff = top - bottom,
variable = ifelse(temp_diff < -0.1 & top <= 4, 'IceOn', 'IceOff')) |>
#surface cooler than bottom (within error (0.1) of the sensors)
dplyr::select(datetime, site_id, variable) |>
tidyr::pivot_longer(cols = variable,
names_to = 'variable',
values_to = 'observation')
#when does the value change from ice on to off? (vice versa)
rle_ice <- rle(current_on_off$observation)
current_ice_df <- data.frame(variable = rle_ice$values, # this is whether the ice is on or off
length = rle_ice$lengths) |>  # how long is the run of this condition?
dplyr::mutate(end_n = cumsum(length), # cumsum is the index of the end of each run
start_n = (end_n - length+1),# this the index of the start of the run
datetime = lubridate::as_date(current_on_off$datetime[start_n])) |>  # relates this the date
dplyr::select(variable, datetime) |>
dplyr::mutate(site_id = current_df$site_id[1],
observation = 1) |> # these are all where ice on/off is occuring
dplyr::select(site_id, datetime, variable, observation)
message('Current file ready')
# read in historical data file
# EDI
infile <- tempfile()
try(download.file(historic_file, infile, method="curl"))
if (is.na(file.size(infile))) download.file(historic_file,infile,method="auto")
historic_df <- readr::read_csv(infile, show_col_types = F) |>
dplyr::mutate(site_id = ifelse(Reservoir == 'FCR', 'fcre',
ifelse(Reservoir == 'BVR', 'bvre', NA))) |>
dplyr::filter(site_id == current_df$site_id[1])
historic_ice_df <- historic_df |>
dplyr::select(site_id, Date, IceOn, IceOff) |>
tidyr::pivot_longer(names_to = 'variable',
values_to = 'observation',
cols = IceOn:IceOff) |>
dplyr::rename(datetime = Date)
message('EDI file ready')
## manipulate the data files to match each other
# dates
period <- dplyr::bind_rows(historic_ice_df, current_ice_df) |>
dplyr::summarise(first = min(datetime),
last = max(datetime))
# get all the days to fill in with 0
all_dates <- expand.grid(datetime = seq.Date(period$first,
period$last, by = 'day'),
variable = c('IceOn', 'IceOff'),
site_id = current_df$site_id[1])
## bind the two files using row.bind()
final_df <- dplyr::bind_rows(historic_ice_df, current_ice_df) |>
dplyr::full_join(all_dates, by = dplyr::join_by(site_id, datetime, variable)) |>
dplyr::arrange(site_id, datetime) |>
dplyr::mutate(observation = ifelse(is.na(observation), 0, observation))
## Match data to flare targets file
# Use pivot_longer to create a long-format table
# for time specific - use midnight UTC values for daily
# for hourly
## return dataframe formatted to match FLARE targets
return(final_df)
}
current_files <- c("https://raw.githubusercontent.com/FLARE-forecast/BVRE-data/bvre-platform-data-qaqc/bvre-waterquality_L1.csv",
"https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-catwalk-data-qaqc/fcre-waterquality_L1.csv")
historic_files <- c("https://pasta.lternet.edu/package/data/eml/edi/456/5/ebfaad16975326a7b874a21beb50c151",
"https://pasta.lternet.edu/package/data/eml/edi/456/5/ebfaad16975326a7b874a21beb50c151")
bvr_ice_data <- target_IceCover_binary(current_file = current_files[1], historic_file = historic_files[1])
fcr_ice_data <- target_IceCover_binary(current_file = current_files[2], historic_file = historic_files[2])
combined_ice_data <- dplyr::bind_rows(bvr_ice_data, fcr_ice_data)
write.csv(combined_ice_data, "C:/Users/13188/Desktop/Data_repository/ice_L1.csv", row.names = FALSE)
View(combined_ice_data)
View(combined_ice_data)
ice <- read_csv("C:/Users/13188/Desktop/Data_repository/ice_L1.csv") |>
filter(observation == 1 & site_id == "fcre")
library(tidyverse)
ice <- read_csv("C:/Users/13188/Desktop/Data_repository/ice_L1.csv") |>
filter(observation == 1 & site_id == "fcre")
View(ice)
ice <- read_csv("C:/Users/13188/Desktop/Data_repository/ice_L1.csv") |>
filter(observation == 1 & site_id == "fcre" & datetime >= "2020-01-01")
View(fcr_ice_data)
View(fcr_ice_data)
target_IceCover_binary <- function(current_file, historic_file){
## read in current data file
# Github, Googlesheet, etc.
current_df <- readr::read_csv(current_file, show_col_types = F)|>
dplyr::filter(Site == 50) |>
dplyr::select(Reservoir, DateTime,
dplyr::starts_with('ThermistorTemp')) |>
tidyr::pivot_longer(cols = starts_with('ThermistorTemp'),
names_to = 'depth',
names_prefix = 'ThermistorTemp_C_',
values_to = 'observation') |>
dplyr::mutate(Reservoir = ifelse(Reservoir == 'FCR',
'fcre',
ifelse(Reservoir == 'BVR',
'bvre', NA)),
datetime = lubridate::as_datetime(paste0(format(DateTime, "%Y-%m-%d %H"), ":00:00"))) |>
dplyr::group_by(Reservoir, datetime, depth) |>
dplyr::summarise(observation = mean(observation, na.rm = T),
.groups = 'drop') |>
dplyr::rename(site_id = Reservoir)
# the depths used to assess will change depending on the current depth of FCR
depths_use <- current_df |>
dplyr::mutate(depth = ifelse(depth == "surface", 0.1, depth)) |>
na.omit() |>
dplyr::group_by(datetime) |>
dplyr::summarise(top = min(as.numeric(depth)),
bottom = max(as.numeric(depth))) |>
tidyr::pivot_longer(cols = top:bottom,
values_to = 'depth')
current_on_off <- current_df |>
dplyr::mutate(depth = as.numeric(ifelse(depth == "surface", 0.1, depth))) |>
dplyr::right_join(depths_use, by = c('datetime', 'depth')) |>
dplyr::select(-depth) |>
tidyr::pivot_wider(names_from = name,
values_from = observation) |>
# Ice defined as when the top is cooler than the bottom, and temp below 4 oC
dplyr::mutate(temp_diff = top - bottom,
variable = ifelse(temp_diff < -0.1 & top <= 4, 'IceOn', 'IceOff')) |>
#surface cooler than bottom (within error (0.1) of the sensors)
dplyr::select(datetime, site_id, variable) |>
tidyr::pivot_longer(cols = variable,
names_to = 'variable',
values_to = 'observation')
#when does the value change from ice on to off? (vice versa)
rle_ice <- rle(current_on_off$observation)
current_ice_df <- data.frame(variable = rle_ice$values, # this is whether the ice is on or off
length = rle_ice$lengths) |>  # how long is the run of this condition?
dplyr::mutate(end_n = cumsum(length), # cumsum is the index of the end of each run
start_n = (end_n - length+1),# this the index of the start of the run
datetime = lubridate::as_date(current_on_off$datetime[start_n])) |>  # relates this the date
dplyr::select(variable, datetime) |>
dplyr::mutate(site_id = current_df$site_id[1],
observation = 1) |> # these are all where ice on/off is occuring
dplyr::select(site_id, datetime, variable, observation)
message('Current file ready')
# read in historical data file
# EDI
infile <- tempfile()
try(download.file(historic_file, infile, method="curl"))
if (is.na(file.size(infile))) download.file(historic_file,infile,method="auto")
historic_df <- readr::read_csv(infile, show_col_types = F) |>
dplyr::mutate(site_id = ifelse(Reservoir == 'FCR', 'fcre',
ifelse(Reservoir == 'BVR', 'bvre', NA))) |>
dplyr::filter(site_id == current_df$site_id[1])
historic_ice_df <- historic_df |>
dplyr::select(site_id, Date, IceOn, IceOff) |>
tidyr::pivot_longer(names_to = 'variable',
values_to = 'observation',
cols = IceOn:IceOff) |>
dplyr::rename(datetime = Date)
message('EDI file ready')
## manipulate the data files to match each other
# dates
period <- dplyr::bind_rows(historic_ice_df, current_ice_df) |>
dplyr::summarise(first = min(datetime),
last = max(datetime))
# get all the days to fill in with 0
all_dates <- expand.grid(datetime = seq.Date(period$first,
period$last, by = 'day'),
variable = c('IceOn', 'IceOff'),
site_id = current_df$site_id[1])
## bind the two files using row.bind()
final_df <- dplyr::bind_rows(historic_ice_df, current_ice_df) |>
dplyr::full_join(all_dates, by = dplyr::join_by(site_id, datetime, variable)) |>
dplyr::arrange(site_id, datetime) |>
dplyr::mutate(observation = ifelse(is.na(observation), 0, observation))
## Match data to flare targets file
# Use pivot_longer to create a long-format table
# for time specific - use midnight UTC values for daily
# for hourly
## return dataframe formatted to match FLARE targets
return(final_df)
}
current_files <- c("https://raw.githubusercontent.com/FLARE-forecast/BVRE-data/bvre-platform-data-qaqc/bvre-waterquality_L1.csv",
"https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-catwalk-data-qaqc/fcre-waterquality_L1.csv")
historic_files <- c("https://pasta.lternet.edu/package/data/eml/edi/456/5/ebfaad16975326a7b874a21beb50c151",
"https://pasta.lternet.edu/package/data/eml/edi/456/5/ebfaad16975326a7b874a21beb50c151")
bvr_ice_data <- target_IceCover_binary(current_file = current_files[1], historic_file = historic_files[1])
fcr_ice_data <- target_IceCover_binary(current_file = current_files[2], historic_file = historic_files[2])
combined_ice_data <- dplyr::bind_rows(bvr_ice_data, fcr_ice_data)
write.csv(combined_ice_data, "C:/Users/13188/Desktop/Data_repository/ice_L1.csv", row.names = FALSE)
