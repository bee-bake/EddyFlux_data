ylab("No of days")+
xlab("Half hours")+
geom_bar()
hh_count_alex <- hh_flux_noNA %>% filter(date <= "2022-04-30") |>
group_by(date) %>%
summarise(frequency = n())
ggplot(hh_count_alex, aes(x = frequency)) +
ylab("No of days")+
xlab("Half hours")+
geom_bar()
#Using 29 half hourly values as our cut-off
post_cutoff <- hh_merged %>% filter(frequency >= 29)
View(hh_merged)
#Using 29 half hourly values as our cut-off
data_loss <- hh_merged %>%
group_by(frequency)%>%
summarise(date = n())
View(data_loss)
data_loss$total <- date*frequency
data_loss$total <- data_loss$date*data_loss$frequency
data_loss$total_hh <- data_loss$date*data_loss$frequency
data_loss$proportion <- data_loss$total_hh/sum(data_loss$total_hh)
data_loss$proportion <- data_loss$total_hh*100/sum(data_loss$total_hh)
#Using 29 half hourly values as our cut-off
data_loss <- hh_merged %>%
group_by(frequency)%>%
summarise(date = n())
data_loss$total_hh <- data_loss$date*data_loss$frequency
data_loss$proportion <- data_loss$total_hh*100/sum(data_loss$total_hh)
ggplot(aes(x = frequency, y = proportion), data = data_loss) +
ylab("No of days with cut-off at zero")+
xlab("Half hours")+
geom_line()
View(ec_full)
#Using 29 half hourly values as our cut-off
data_loss <- hh_merged %>%
group_by(frequency)%>%
summarise(days = n())
data_loss$total_hh <- data_loss$date*data_loss$frequency
#Using 29 half hourly values as our cut-off
data_loss <- hh_merged %>%
group_by(frequency)%>%
summarise(date = n())
data_loss$total_hh <- data_loss$date*data_loss$frequency
data_loss$proportion <- data_loss$total_hh*100/sum(data_loss$total_hh)
ggplot(aes(x = frequency, y = proportion), data = data_loss) +
ylab("No of days with cut-off at zero")+
xlab("Half hours")+
geom_line()
#Using 29 half hourly values as our cut-off
data_loss <- hh_merged %>%
group_by(frequency)%>%
summarise(date = n())
data_loss$total_hh <- data_loss$date*data_loss$frequency
data_loss$proportion <- data_loss$total_hh*100/sum(data_loss$total_hh)
for (i in 1:length(data_loss$frequency)) {
data_loss$prop = proportion[1] + proportion[2]
}
#Using 29 half hourly values as our cut-off
data_loss <- hh_merged %>%
group_by(frequency)%>%
summarise(date = n())
data_loss$total_hh <- data_loss$date*data_loss$frequency
data_loss$proportion <- data_loss$total_hh*100/sum(data_loss$total_hh)
for (i in 1:length(data_loss$frequency)) {
data_loss$prop = data_loss$proportion[1] + data_loss$proportion[2]
}
ggplot(aes(x = frequency, y = prop), data = data_loss) +
ylab("No of days with cut-off at zero")+
xlab("Half hours")+
geom_line()
#Using 29 half hourly values as our cut-off
data_loss <- hh_merged %>%
group_by(frequency)%>%
summarise(date = n())
data_loss$total_hh <- data_loss$date*data_loss$frequency
data_loss$proportion <- data_loss$total_hh*100/sum(data_loss$total_hh)
for (i in 1:length(data_loss$frequency)) {
data_loss$prop = data_loss$proportion[i-1] + data_loss$proportion[i]
}
#Using 29 half hourly values as our cut-off
data_loss <- hh_merged %>%
group_by(frequency)%>%
summarise(date = n())
data_loss$total_hh <- data_loss$date*data_loss$frequency
data_loss$proportion <- data_loss$total_hh*100/sum(data_loss$total_hh)
for (i in 1:length(data_loss$frequency)) {
data_loss$prop[i] = data_loss$proportion[i-1] + data_loss$proportion[i]
}
#Using 29 half hourly values as our cut-off
data_loss <- hh_merged %>%
group_by(frequency)%>%
summarise(date = n())
data_loss$total_hh <- data_loss$date*data_loss$frequency
data_loss$proportion <- data_loss$total_hh*100/sum(data_loss$total_hh)
for (i in 2:length(data_loss$frequency)) {
data_loss$prop[i] = data_loss$proportion[i-1] + data_loss$proportion[i]
}
#Using 29 half hourly values as our cut-off
data_loss <- hh_merged %>%
group_by(frequency)%>%
summarise(date = n())
data_loss$proportion <- data_loss$date/sum(data_loss$date)
ggplot(aes(x = frequency, y = prop), data = data_loss) +
ylab("No of days with cut-off at zero") +
xlab("Half hours") +
geom_line()
#Using 29 half hourly values as our cut-off
data_loss <- hh_merged %>%
group_by(frequency)%>%
summarise(date = n())
data_loss$proportion <- data_loss$date/sum(data_loss$date)
ggplot(aes(x = frequency, y = proportion), data = data_loss) +
ylab("No of days with cut-off at zero") +
xlab("Half hours") +
geom_line()
#Using 29 half hourly values as our cut-off
data_loss <- hh_merged %>%
group_by(frequency)%>%
summarise(date = n())
data_loss$proportion <- data_loss$date/sum(data_loss$date)
ggplot(aes(x = frequency, y = proportion), data = data_loss) +
ylab("No of days with cut-off at zero") +
xlab("Half hours") +
geom_line()
url <- "https://renc.osn.xsede.org/bio230121-bucket01/vera4cast/targets/project_id=vera4cast/duration=P1D/daily-insitu-targets.csv.gz"
library(tidyverse)
targets <- read_csv(url, show_col_types = FALSE)
View(targets)
googlesheets4::gs4_deauth()
target_metadata <- googlesheets4::read_sheet("https://docs.google.com/spreadsheets/d/1fOWo6zlcWA8F6PmRS9AD6n1pf-dTWSsmGKNpaX3yHNE/edit?usp=sharing")
site_list <- read_csv("https://raw.githubusercontent.com/LTREB-reservoirs/vera4cast/main/vera4cast_field_site_metadata.csv", show_col_types = FALSE)
url2 <- "https://renc.osn.xsede.org/bio230121-bucket01/vera4cast/targets/project_id=vera4cast/duration=P1D/daily-inflow-targets.csv.gz"
inflow_targets <- read_csv(url2, show_col_types = FALSE)
glimpse(inflow_targets)
View(inflow_targets)
library(polynom)
x_p <- unlist(one_day_summarized$s_size)
f_p <- unlist(one_day_summarized$sd)
#fitting third order polynomial regression
poly_eq <- lm(f_p ~ poly(x_p, 3, raw=TRUE))
summary(poly_eq)
coeff <- coef(poly_eq)
#Plot the function
f <- function(x_p) {
return(coeff[1] + coeff[2]*x_p + coeff[3]*x_p^2 + coeff[4]*x_p^3)
}
dat <- tibble(x_p, f_p)
ggplot(dat, aes(x=x_p, y=f_p)) +
geom_point(size=1, col='blue') +
stat_function(fun = f, linewidth=1, alpha=0.4) + xlab("half-hourly values") + ylab("standard deviation")
ggplot(df, aes(x = x, y = y)) +
geom_line()
ggplot(df, aes(x = x, y = y)) +
geom_line() + xlab("half-hourly values") + ylab("standard deviation")
ggplot(df, aes(x = x, y = y)) +
geom_line() + xlab("half-hourly values") + ylab("second derivative")
knitr::opts_chunk$set(echo = TRUE)
#Count the number of half-hourly values each day
hh_flux_noNA <- na.omit(hh_flux)
knitr::opts_chunk$set(echo = TRUE)
#Clear workspace
rm(list=ls())
#Load packages
library(tidyverse)
library(lubridate)
library(ggplot2)
# Read compiled file: From Eddy Pro using basic processing
# Original file from Brenda on 11 May 2021
# Light data cleaning using EddyPro_CleanUp.R
dt1 <-read_csv("https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataNotYetUploadedToEDI/EddyFlux_Processing/EddyPro_Cleaned_L1.csv")
# Check here on EDI an make sure you are using the most up to date file.
# https://portal.edirepository.org/nis/mapbrowse?packageid=edi.1061.2
# if not pulll in the file path from EDI
# read in the data file downloaded from EDI
dt2 <-read_csv("https://pasta.lternet.edu/package/data/eml/edi/1061/2/f837d12dc12ab37a6772598578875e00")
#Create a full dataset
ec_full <- bind_rows(dt2, dt1)
#Extract the mean flux values
hh_flux <-  ec_full %>%
select(c('date', 'time','co2_flux_umolm2s','ch4_flux_umolm2s'))
#Count how many data points are available at each half hour interval
availability <- hh_flux %>% group_by(time) %>%
select(co2_flux_umolm2s, ch4_flux_umolm2s) %>%
summarise(co2_available = 100-sum(is.na(co2_flux_umolm2s))/n()*100,
ch4_available = 100-sum(is.na(ch4_flux_umolm2s))/n()*100,
co2_n = n() - sum(is.na(co2_flux_umolm2s)),
ch4_n = n() - sum(is.na(ch4_flux_umolm2s)))
availability
#Plot the data availability by half-hour
avail <- availability %>%
pivot_longer(cols = co2_available:ch4_available, names_to = "variable",
values_to = "flux_value")
ggplot()+
geom_point(data =avail, aes(x=time, y=flux_value, color = variable))+
ylab("% of available half-hourly data")
#calculate the mean flux values by half hour
hh_flux2 = subset(hh_flux, select = -c(date))
hh_mean <- hh_flux2 %>%
group_by(time) %>%
summarise_all(mean, na.rm = TRUE)
#Plot the values
ggplot()+
geom_point(data =hh_mean, aes(x=time, y=co2_flux_umolm2s))+
ylab("CO2")+
ggtitle("Half hourly mean values across the day")
ggplot()+
geom_point(data =hh_mean, aes(x=time, y=ch4_flux_umolm2s))+
ylab("CH4")+
ggtitle("Half hourly mean values across the day")
#Count the number of half-hourly values each day
hh_flux_noNA <- na.omit(hh_flux)
hh_count <- hh_flux_noNA %>%
group_by(date) %>%
summarise(frequency = n())
ggplot(hh_count, aes(x = frequency)) +
ylab("No of days")+
xlab("Half hours")+
geom_bar()
knitr::opts_chunk$set(echo = TRUE)
#Clear workspace
rm(list=ls())
#Load packages
library(tidyverse)
library(lubridate)
library(ggplot2)
# Read compiled file: From Eddy Pro using basic processing
# Original file from Brenda on 11 May 2021
# Light data cleaning using EddyPro_CleanUp.R
dt1 <-read_csv("https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataNotYetUploadedToEDI/EddyFlux_Processing/EddyPro_Cleaned_L1.csv")
# Read compiled file: From Eddy Pro using basic processing
# Original file from Brenda on 11 May 2021
# Light data cleaning using EddyPro_CleanUp.R
dt1 <-read_csv("https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataNotYetUploadedToEDI/EddyFlux_Processing/EddyPro_Cleaned_L1.csv")
View(dt1)
# Check here on EDI an make sure you are using the most up to date file.
# https://portal.edirepository.org/nis/mapbrowse?packageid=edi.1061.2
# if not pulll in the file path from EDI
# read in the data file downloaded from EDI
dt2 <-read_csv("https://pasta.lternet.edu/package/data/eml/edi/1061/3/e0976e7a6543fada4cbf5a1bb168713b")
#Create a full dataset
ec_full <- dt2
#Extract the mean flux values
hh_flux <-  ec_full %>%
select(c('date', 'time','co2_flux_umolm2s','ch4_flux_umolm2s'))
#Count how many data points are available at each half hour interval
availability <- hh_flux %>% group_by(time) %>%
select(co2_flux_umolm2s, ch4_flux_umolm2s) %>%
summarise(co2_available = 100-sum(is.na(co2_flux_umolm2s))/n()*100,
ch4_available = 100-sum(is.na(ch4_flux_umolm2s))/n()*100,
co2_n = n() - sum(is.na(co2_flux_umolm2s)),
ch4_n = n() - sum(is.na(ch4_flux_umolm2s)))
availability
#Plot the data availability by half-hour
avail <- availability %>%
pivot_longer(cols = co2_available:ch4_available, names_to = "variable",
values_to = "flux_value")
ggplot()+
geom_point(data =avail, aes(x=time, y=flux_value, color = variable))+
ylab("% of available half-hourly data")
#calculate the mean flux values by half hour
hh_flux2 = subset(hh_flux, select = -c(date))
hh_mean <- hh_flux2 %>%
group_by(time) %>%
summarise_all(mean, na.rm = TRUE)
#Plot the values
ggplot()+
geom_point(data =hh_mean, aes(x=time, y=co2_flux_umolm2s))+
ylab("CO2")+
ggtitle("Half hourly mean values across the day")
ggplot()+
geom_point(data =hh_mean, aes(x=time, y=ch4_flux_umolm2s))+
ylab("CH4")+
ggtitle("Half hourly mean values across the day")
#Count the number of half-hourly values each day
hh_flux_noNA <- na.omit(hh_flux)
hh_count <- hh_flux_noNA %>%
group_by(date) %>%
summarise(frequency = n())
ggplot(hh_count, aes(x = frequency)) +
ylab("No of days")+
xlab("Half hours")+
geom_bar()
#Create a full dataset
ec_full <- bind_rows(dt2, dt1)
#Extract the mean flux values
hh_flux <-  ec_full %>%
select(c('date', 'time','co2_flux_umolm2s','ch4_flux_umolm2s'))
#Count how many data points are available at each half hour interval
availability <- hh_flux %>% group_by(time) %>%
select(co2_flux_umolm2s, ch4_flux_umolm2s) %>%
summarise(co2_available = 100-sum(is.na(co2_flux_umolm2s))/n()*100,
ch4_available = 100-sum(is.na(ch4_flux_umolm2s))/n()*100,
co2_n = n() - sum(is.na(co2_flux_umolm2s)),
ch4_n = n() - sum(is.na(ch4_flux_umolm2s)))
availability
#Plot the data availability by half-hour
avail <- availability %>%
pivot_longer(cols = co2_available:ch4_available, names_to = "variable",
values_to = "flux_value")
ggplot()+
geom_point(data =avail, aes(x=time, y=flux_value, color = variable))+
ylab("% of available half-hourly data")
#calculate the mean flux values by half hour
hh_flux2 = subset(hh_flux, select = -c(date))
hh_mean <- hh_flux2 %>%
group_by(time) %>%
summarise_all(mean, na.rm = TRUE)
#Plot the values
ggplot()+
geom_point(data =hh_mean, aes(x=time, y=co2_flux_umolm2s))+
ylab("CO2")+
ggtitle("Half hourly mean values across the day")
ggplot()+
geom_point(data =hh_mean, aes(x=time, y=ch4_flux_umolm2s))+
ylab("CH4")+
ggtitle("Half hourly mean values across the day")
#Count the number of half-hourly values each day
hh_flux_noNA <- na.omit(hh_flux)
hh_count <- hh_flux_noNA %>%
group_by(date) %>%
summarise(frequency = n())
ggplot(hh_count, aes(x = frequency)) +
ylab("No of days")+
xlab("Half hours")+
geom_bar()
#Calculate the mean flux values by day
hh_dmean <- hh_flux_noNA %>%
select(c(date, co2_flux_umolm2s,ch4_flux_umolm2s)) %>%
group_by(date) %>%
summarise_all(list(mean, sd)) %>%
rename(
co2_mean = co2_flux_umolm2s_fn1,
co2_sd = co2_flux_umolm2s_fn2,
ch4_mean = ch4_flux_umolm2s_fn1,
ch4_sd = ch4_flux_umolm2s_fn2,
)
#Join two dataframes with mean values and count values
hh_merged <- merge(x = hh_dmean, y = hh_count, by = "date", all = TRUE)
#Plot the mean values versus the number of half hourly values present
ggplot()+
geom_point(data = hh_merged, aes(x=frequency, y=co2_mean, color = frequency))+
xlab("Half hours")+
ylab("mean_CO2")
ggplot()+
geom_point(data = hh_merged, aes(x=frequency, y=ch4_mean, color = frequency))+
xlab("Half hours")+
ylab("mean_CH4")
View(ec_full)
#Data processed with Brenda's script
dt3 <- read_csv("Data/DataNotYetUploadedToEDI/EddyFlux_Processing/2024-04-02_EC_processed_withBDScript.csv")
#Data processed with Brenda's script
dt3 <- read_csv("https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataNotYetUploadedToEDI/EddyFlux_Processing/2024-04-02_EC_processed_withBDScript.csv")
#Data processed with Brenda's script
dt3 <- read_csv("C:\Users\13188\Desktop\Reservoir\Reservoirs\Data\DataNotYetUploadedToEDI\EddyFlux_Processing\2024-04-02_EC_processed_withBDScript.csv")
#Data processed with Brenda's script
dt3 <- read_csv("C:/Users/13188/Desktop/Reservoir/Reservoirs/Data/DataNotYetUploadedToEDI/EddyFlux_Processing/2024-04-02_EC_processed_withBDScript.csv")
View(dt3)
View(dt3)
install.packages('pacman')
pacman::p_load("tidyverse","lubridate")
generate_EddyFlux_ghg_targets_function <- function(flux_current_data_file,
flux_edi_data_file,
met_current_data_file,
met_edi_data_file){
# Things to figure out is how many fluxes are needed for a good daily flux.
# Right now it is just a daily average no matter if it is one observation or 48
# functions we need for despike
source("https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataNotYetUploadedToEDI/EddyFlux_Processing/despike.R")
## Read in the data files
## read in EddyFlux summary files from the current data file which is found on GitHub
dt1 <-read_csv(flux_current_data_file)
# read in historical data file
# EDI
# read in the data file downloaded from EDI
dt2 <-read_csv(flux_edi_data_file)
# combine the historic and the current data file
ec <- dt2%>%
bind_rows(.,dt1)%>%
distinct()
# Format time
# make a datetime column and read in with original timezone
ec$datetime <- paste0(ec$date, " ",ec$time)
# Set timezone as America/New_York because that's what it is in and then convert to EST
ec$datetime <- force_tz(ymd_hms(ec$datetime), tzone = "America/New_York")
# convert from Eastern/US with daylight savings observed to EST which does not.
ec$datetime <- with_tz(ec$datetime, tzone = "EST")
#### Reading in data from the Met Station for QAQCing when raining
# Load data Meteorological data from EDI
# Read in Met file from EDI
met_all <- read_csv(met_edi_data_file,
col_select=c("DateTime","Rain_Total_mm"))%>%
mutate(DateTime = force_tz(DateTime, tzone="EST"))%>%
# Start timeseries on the 00:15:00 to facilitate 30-min averages
filter(DateTime >= ymd_hms("2020-04-04 00:15:00", tz="EST"))
# Bind files together if need to use current file
met_curr <- read_csv(met_current_data_file,
col_select=c("DateTime","Rain_Total_mm"))%>%
mutate(DateTime = force_tz(DateTime, tzone="EST"))
met_all <- dplyr::bind_rows(met_curr, met_all) # bind everything together
# Start timeseries on the 00:15:00 to facilitate 30-min averages
# Select data every 30 minutes from Jan 2020 to end of met data
met_all$Breaks <- cut(met_all$DateTime,breaks = "30 mins",right=FALSE)
met_all$Breaks <- parse_date_time(met_all$Breaks, orders=c("ymd", "ymd HMS"), tz="EST")
# Sum met data to the 30 min mark (for Total Rain and Total PAR)
met_2 <- met_all %>%
select(DateTime,Rain_Total_mm,Breaks) %>%
group_by(Breaks) %>%
summarise_if(is.numeric,sum,na.rm=TRUE) %>%
ungroup()%>%
mutate(DateTime=Breaks - 900)%>%
rename(datetime = DateTime,
Rain_sum = Rain_Total_mm)
ec2 <- left_join(ec, met_2, by='datetime')
# convert time to UTC
ec2 <- ec2 |>
dplyr::mutate(datetime_utc = with_tz(datetime, tz = 'UTC'),
date = as.Date(datetime_utc))
# Filter out wind directions that are BEHIND the catwalk
# I.e., only keep data that is IN FRONT of the catwalk for both EC and Met data
ec_filt <- ec2 %>% dplyr::filter(wind_dir < 80 | wind_dir > 250)
# Remove values that are greater than abs(100)
# NOTE: Updated from Brenda's code to use abs(100); instead of -70 to 100 filtering
# Waldo et al. 2021 used: values greater than abs(15000)
ec_filt$co2_flux_umolm2s <- ifelse(ec_filt$co2_flux_umolm2s > 100 | ec_filt$co2_flux_umolm2s < -100, NA, ec_filt$co2_flux_umolm2s)
# Remove CO2 data if QC >= 2 (aka: data that has been flagged by Eddy Pro)
ec_filt$co2_flux_umolm2s <- ifelse(ec_filt$qc_co2_flux >= 2, NA, ec_filt$co2_flux_umolm2s)
# Additionally remove CO2 data when H and LE > 2 (following CH4 filtering)
ec_filt$co2_flux_umolm2s <- ifelse(ec_filt$qc_co2_flux==1 & ec_filt$qc_LE>=2, NA, ec_filt$co2_flux_umolm2s)
ec_filt$co2_flux_umolm2s <- ifelse(ec_filt$qc_co2_flux==1 & ec_filt$qc_H>=2, NA, ec_filt$co2_flux_umolm2s)
# Remove large CH4 values
# Remove values that are greater than abs(0.25)
# NOTE: Updated from Brenda's code to use abs(0.25)
# Waldo et al. 2021 used: values greater than abs(500)
ec_filt$ch4_flux_umolm2s <- ifelse(ec_filt$ch4_flux_umolm2s >= 0.25 | ec_filt$ch4_flux_umolm2s <= -0.25, NA, ec_filt$ch4_flux_umolm2s)
# Remove ch4 values when signal strength < 20
ec_filt$ch4_flux_umolm2s <- ifelse(ec_filt$rssi_77_mean < 20, NA, ec_filt$ch4_flux_umolm2s)
# Remove CH4 data if QC >= 2
ec_filt$ch4_flux_umolm2s <- ifelse(ec_filt$qc_ch4_flux >=2, NA, ec_filt$ch4_flux_umolm2s)
# Additionally, remove CH4 when other parameters are QA/QC'd
# Following Waldo et al. 2021: Remove additional ch4 flux data
# (aka: anytime ch4_qc flag = 1 & another qc_flag =2, remove)
ec_filt$ch4_flux_umolm2s <- ifelse(ec_filt$qc_ch4_flux==1 & ec_filt$qc_co2_flux>=2, NA, ec_filt$ch4_flux_umolm2s)
ec_filt$ch4_flux_umolm2s <- ifelse(ec_filt$qc_ch4_flux==1 & ec_filt$qc_LE>=2, NA, ec_filt$ch4_flux_umolm2s)
ec_filt$ch4_flux_umolm2s <- ifelse(ec_filt$qc_ch4_flux==1 & ec_filt$qc_H>=2, NA, ec_filt$ch4_flux_umolm2s)
# Check QC for H and LE
# Removing qc >= 2 for H and LE
ec_filt$H_wm2 <- ifelse(ec_filt$qc_H >= 2, NA, ec_filt$H_wm2)
ec_filt$LE_wm2 <- ifelse(ec_filt$qc_LE >= 2, NA, ec_filt$LE_wm2)
# Remove high H values: greater than abs(200)
# NOTE: Updated to have same upper and lower magnitude bound
# Waldo et al. 2021 used abs of 200 for H
ec_filt$H_wm2 <- ifelse(ec_filt$H_wm2 >= 200 | ec_filt$H_wm2 <= -200, NA, ec_filt$H_wm2)
# Remove high LE values: greater than abs(500)
# NOTE: Updated to have same upper and lower magnitude bounds
# Waldo et al. 2021 used abs of 1000 for LE
ec_filt$LE_wm2 <- ifelse(ec_filt$LE_wm2 >= 500 | ec_filt$LE_wm2 <= -500, NA, ec_filt$LE_wm2)
# Remove CH4 when it rains
ec_filt$ch4_flux_umolm2s <- ifelse(ec_filt$Rain_sum > 0, NA, ec_filt$ch4_flux_umolm2s)
# Remove CH4 data when thermocouple was not working (apr 05 - apr 25) # ABP find for 2023
ec_filt$ch4_flux_umolm2s <- ifelse(ec_filt$datetime >= '2021-04-05' & ec_filt$datetime <= '2021-04-25',
NA, ec_filt$ch4_flux_umolm2s)
eddy_fcr <- ec_filt
# Despike NEE (CO2 flux) and CH4. Use the function sourced at the beginning of the script
# Calculate low, medium, and high data flags
flag <- spike_flag(eddy_fcr$co2_flux_umolm2s,z = 7)
NEE_low <- ifelse(flag == 1, NA, eddy_fcr$co2_flux_umolm2s)
flag <- spike_flag(eddy_fcr$co2_flux_umolm2s,z = 5.5)
NEE_medium <- ifelse(flag == 1, NA, eddy_fcr$co2_flux_umolm2s)
flag <- spike_flag(eddy_fcr$co2_flux_umolm2s,z = 4)
NEE_high <- ifelse(flag == 1, NA, eddy_fcr$co2_flux_umolm2s)
# Combine all flagged data into the data frame but only keep medium one
eddy_fcr$CO2_med_flux <- NEE_medium
#Despike CH4 flux
flag <- spike_flag(eddy_fcr$ch4_flux_umolm2s,z = 7)
CH4_low <- ifelse(flag == 1, NA, eddy_fcr$ch4_flux_umolm2s)
flag <- spike_flag(eddy_fcr$ch4_flux_umolm2s,z = 5.5)
CH4_medium <- ifelse(flag == 1, NA, eddy_fcr$ch4_flux_umolm2s)
flag <- spike_flag(eddy_fcr$ch4_flux_umolm2s,z = 4)
CH4_high <- ifelse(flag == 1, NA, eddy_fcr$ch4_flux_umolm2s)
# Combine all flagged data into the data frame but only keep the medium one
eddy_fcr$ch4_med_flux <- CH4_medium
# Filter out all the values (x_peak) that are out of the reservoir
eddy_fcr$footprint_flag <- ifelse(eddy_fcr$wind_dir >= 15 & eddy_fcr$wind_dir <= 90 & eddy_fcr$x_peak_m >= 40, 1,
ifelse(eddy_fcr$wind_dir < 15 & eddy_fcr$wind_dir > 327 & eddy_fcr$x_peak_m > 120, 1,
ifelse(eddy_fcr$wind_dir < 302 & eddy_fcr$wind_dir >= 250 & eddy_fcr$x_peak_m > 50, 1, 0)))
# Remove flagged data
targets_df <- eddy_fcr %>%
filter(footprint_flag == 0)%>% # filter out so it is the smallest footprint
select(date, CO2_med_flux, ch4_med_flux)%>%
dplyr::rename(co2flux_umolm2s_mean = CO2_med_flux,
ch4flux_umolm2s_mean = ch4_med_flux)%>%  # rename columns
group_by(date)%>% # average if there are more than one sample taken during that day
summarise_if(is.numeric, mean, na.rm = TRUE)%>%
ungroup()%>%
drop_na(date)%>% # drop when we have timezone issues with daylight savings
mutate(datetime=(paste0(date," ","00:00:00")))%>%
#drop_na(datetime) %>%
mutate(Reservoir='fcre')%>% # change the name to the the reservoir code for FLARE
mutate(Depth_m = NA)%>%
select(-date)%>%
rename(site_id=Reservoir, # rename the columns for standard notation
depth=Depth_m)%>%
pivot_longer(cols=c(co2flux_umolm2s_mean, ch4flux_umolm2s_mean), # make the wide data frame into a long one so each observation has a depth
names_to='variable',
values_to='observation')%>%
select(c('datetime', 'site_id', 'depth', "observation", 'variable')) # rearrange order of columns
## return dataframe formatted to match FLARE targets
return(targets_df)
}
source("generate_EddyFlux_ghg_targets_function.R")
getwd()
