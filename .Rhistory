season_co2 <- season_data_co2 %>%
filter(`!is.na(NEE_uStar_orig)` == "TRUE")
#Create new data frame to equalize the number of rows
new <- data.frame(c("NA", "NA", "NA", "NA", "NA", "NA", "NA", "NA"),
c("NA", "NA", "NA", "NA", "NA", "NA", "NA", "NA"),
c("NA", "NA", "NA", "NA", "NA", "NA", "NA", "NA"),
c("NA", "NA", "NA", "NA", "NA", "NA", "NA", "NA"))
season_co2 <- rbind(season_co2, new)
ggplot(season_co2, mapping=aes(x=month,y=n/season_data_total$`sum(n)`*100,fill=as.factor(year)))+
geom_hline(yintercept = 10, color = "lightgrey")+
geom_hline(yintercept = 20, color = "lightgrey")+
geom_hline(yintercept = 30, color = "lightgrey")+
geom_bar(stat="identity",position=position_dodge())+
ylab(expression(~Percent~CO[2]~Data)) +
xlab("Month")+
scale_fill_brewer(palette = "Dark2")+
ylim(0,35)+
guides(fill=guide_legend(title="Year"))+
theme_classic(base_size = 15)
season_ch4 <- season_data_ch4 %>%
filter(`!is.na(ch4_flux_uStar_orig)` == "TRUE")
ggplot(season_ch4, mapping=aes(x=month,y=n/season_data_total$`sum(n)`*100,fill=as.factor(year)))+
geom_hline(yintercept = 10, color = "lightgrey")+
geom_hline(yintercept = 20, color = "lightgrey")+
geom_hline(yintercept = 30, color = "lightgrey")+
geom_bar(stat="identity",position=position_dodge())+
ylab(expression(~Percent~CH[4]~Data)) +
xlab("Month")+
scale_fill_brewer(palette = "Dark2")+
ylim(0,35)+
guides(fill=guide_legend(title="Year"))+
theme_classic(base_size = 15)
ggarrange(season_co2,labels=c("A."),font.label = list(face="plain",size=15))
ggarrange(season_ch4,labels=c("B."),font.label = list(face="plain",size=15))
plot1 <- ggplot(season_co2, mapping=aes(x=month,y=n/season_data_total$`sum(n)`*100,fill=as.factor(year)))+
geom_hline(yintercept = 10, color = "lightgrey")+
geom_hline(yintercept = 20, color = "lightgrey")+
geom_hline(yintercept = 30, color = "lightgrey")+
geom_bar(stat="identity",position=position_dodge())+
ylab(expression(~Percent~CO[2]~Data)) +
xlab("Month")+
scale_fill_brewer(palette = "Dark2")+
ylim(0,35)+
guides(fill=guide_legend(title="Year"))+
theme_classic(base_size = 15)
season_ch4 <- season_data_ch4 %>%
filter(`!is.na(ch4_flux_uStar_orig)` == "TRUE")
plot2 <- ggplot(season_ch4, mapping=aes(x=month,y=n/season_data_total$`sum(n)`*100,fill=as.factor(year)))+
geom_hline(yintercept = 10, color = "lightgrey")+
geom_hline(yintercept = 20, color = "lightgrey")+
geom_hline(yintercept = 30, color = "lightgrey")+
geom_bar(stat="identity",position=position_dodge())+
ylab(expression(~Percent~CH[4]~Data)) +
xlab("Month")+
scale_fill_brewer(palette = "Dark2")+
ylim(0,35)+
guides(fill=guide_legend(title="Year"))+
theme_classic(base_size = 15)
plot_list <- list(plot1, plot2)
ggarrange(plotlist = plot_list, labels = c('A', 'B'), ncol = 2)
ggarrange(plotlist = plot_list, labels = c('A', 'B'), ncol = 2, common.legend = TRUE)
ggarrange(plotlist = plot_list, labels = c('A', 'B'), ncol = 1, common.legend = TRUE)
ggarrange(plotlist = plot_list, labels = c('A', 'B'), ncol = 1, common.legend = TRUE, size = 10)
ggarrange(plotlist = plot_list, labels = c('A', 'B'), ncol = 1, common.legend = TRUE)
plot2 <- ggplot(season_ch4, mapping=aes(x=month,y=n/season_data_total$`sum(n)`*100,fill=as.factor(year)))+
geom_hline(yintercept = 10, color = "lightgrey")+
geom_hline(yintercept = 20, color = "lightgrey")+
geom_hline(yintercept = 30, color = "lightgrey")+
geom_bar(stat="identity",position=position_dodge())+
ylab(expression(~Percent~CH[4]~Data)) +
xlab("")+
scale_fill_brewer(palette = "Dark2")+
ylim(0,35)+
guides(fill=guide_legend(title="Year"))+
theme_classic(base_size = 15)
plot_list <- list(plot1, plot2)
ggarrange(plotlist = plot_list, labels = c('A', 'B'), ncol = 1, common.legend = TRUE)
plot1 <- ggplot(season_co2, mapping=aes(x=month,y=n/season_data_total$`sum(n)`*100,fill=as.factor(year)))+
geom_hline(yintercept = 10, color = "lightgrey")+
geom_hline(yintercept = 20, color = "lightgrey")+
geom_hline(yintercept = 30, color = "lightgrey")+
geom_bar(stat="identity",position=position_dodge())+
ylab(expression(~Percent~CO[2]~Data)) +
xlab("")+
scale_fill_brewer(palette = "Dark2")+
ylim(0,35)+
guides(fill=guide_legend(title="Year"))+
theme_classic(base_size = 15)
season_ch4 <- season_data_ch4 %>%
filter(`!is.na(ch4_flux_uStar_orig)` == "TRUE")
plot2 <- ggplot(season_ch4, mapping=aes(x=month,y=n/season_data_total$`sum(n)`*100,fill=as.factor(year)))+
geom_hline(yintercept = 10, color = "lightgrey")+
geom_hline(yintercept = 20, color = "lightgrey")+
geom_hline(yintercept = 30, color = "lightgrey")+
geom_bar(stat="identity",position=position_dodge())+
ylab(expression(~Percent~CH[4]~Data)) +
xlab("Month")+
scale_fill_brewer(palette = "Dark2")+
ylim(0,35)+
guides(fill=guide_legend(title="Year"))+
theme_classic(base_size = 15)
plot_list <- list(plot1, plot2)
ggarrange(plotlist = plot_list, labels = c('A', 'B'), ncol = 1, common.legend = TRUE)
ggsave("./Fig_Output/SI_Data_Season.jpg",width = 9, height=4, units="in",dpi=320)
## Visualize measured fluxes at the hourly, daily, weekly, and monthly timescale
fcr_hourly <- ec2 %>%
mutate(DateTime = format(as.POSIXct(DateTime, "%Y-%m-%d %H"),"%Y-%m-%d %H")) %>%
mutate(DateTime = as.POSIXct(DateTime, "%Y-%m-%d %H", tz = "EST")) %>%
group_by(DateTime) %>%
mutate(Year = year(DateTime),
Month = month(DateTime),
Day = day(DateTime),
Hour = hour(DateTime)) %>%
summarise(NEE = mean(NEE_uStar_orig, na.rm = TRUE),
NEE_sd = sd(NEE_uStar_orig, na.rm = TRUE),
CH4 = mean(ch4_flux_uStar_orig, na.rm = TRUE),
CH4_sd = sd(ch4_flux_uStar_orig, na.rm = TRUE))
# Calculate min, max, median, mean, standard deviation, and coefficient of variation
# Table S4
fcr_stats <- fcr_hourly %>%
summarise(min_co2 = min(NEE,na.rm = TRUE),
max_co2 = max(NEE,na.rm=TRUE),
med_co2 = median(NEE,na.rm=TRUE),
mean_co2 = mean(NEE,na.rm=TRUE),
sd_co2 = sd(NEE,na.rm=TRUE),
cv_co2 = sd(NEE,na.rm=TRUE)/mean(NEE,na.rm=TRUE)*100,
min_ch4 = min(CH4,na.rm = TRUE),
max_ch4 = max(CH4,na.rm=TRUE),
med_ch4 = median(CH4,na.rm=TRUE),
mean_ch4 = mean(CH4,na.rm=TRUE),
sd_ch4 = sd(CH4,na.rm=TRUE),
cv_ch4 = sd(CH4,na.rm=TRUE)/mean(CH4,na.rm=TRUE)*100)
write.csv(fcr_stats,"./Fig_output/20220506_TableSx_ECStats.csv")
library(readr)
X20220506_TableSx_ECStats <- read_csv("Fig_Output/20220506_TableSx_ECStats.csv")
View(X20220506_TableSx_ECStats)
# Aggregate to daily and calculate the variability (SD) - following script for figures_BD
# data in umolm2s
fcr_daily <- ec2 %>%
mutate(Year = year(DateTime),
Month = month(DateTime),
Day = day(DateTime),
Hour = hour(DateTime)) %>%
dplyr::group_by(Year, Month, Day) %>%
dplyr::summarise(NEE = mean(NEE_uStar_orig, na.rm = TRUE),
NEE_sd = sd(NEE_uStar_orig, na.rm = TRUE),
CH4 = mean(ch4_flux_uStar_orig, na.rm = TRUE),
CH4_sd = sd(ch4_flux_uStar_orig, na.rm = TRUE))
fcr_daily$Date <- as.POSIXct(paste(fcr_daily$Year, fcr_daily$Month, fcr_daily$Day, sep = '-'), "%Y-%m-%d", tz = 'EST')
# Aggregate into weekly
fcr_weekly <- ec2 %>%
mutate(Year = year(DateTime),
Month = month(DateTime),
Week = week(DateTime)) %>%
dplyr::group_by(Year, Week) %>%
dplyr::summarise(NEE = mean(NEE_uStar_orig, na.rm = TRUE),
NEE_sd = sd(NEE_uStar_orig, na.rm = TRUE),
CH4 = mean(ch4_flux_uStar_orig, na.rm = TRUE),
CH4_sd = sd(ch4_flux_uStar_orig, na.rm = TRUE))
fcr_weekly$Date <- make_datetime(year = fcr_weekly$Year) + weeks(fcr_weekly$Week)
# Aggregate to Monthly
fcr_monthly <- ec2 %>%
mutate(Year = year(DateTime),
Month = month(DateTime)) %>%
dplyr::group_by(Year, Month) %>%
dplyr::summarise(NEE = mean(NEE_uStar_orig, na.rm = TRUE),
NEE_sd = sd(NEE_uStar_orig, na.rm = TRUE),
CH4 = mean(ch4_flux_uStar_orig, na.rm = TRUE),
CH4_sd = sd(ch4_flux_uStar_orig, na.rm = TRUE))
fcr_monthly$yearmon <- with(fcr_monthly, sprintf("%d-%02d", Year, Month))
View(fcr_daily)
View(fcr_weekly)
## Load in diffusive fluxes:
## Calculated using: 1b_Diffusive_Fluxes.R
diff_flux <- read.csv("./Data/20220617_diffusive_fluxes_avg.csv") %>%
mutate(DateTime = as.POSIXct(DateTime, "%Y-%m-%d %H:%M:%S", tz = "EST"))
## Load in diffusive fluxes:
## Calculated using: 1b_Diffusive_Fluxes.R
diff_flux <- read.csv("https://pasta-s.lternet.edu/package/data/eml/edi/1102/4/dce28b69ccf1b2b138ad3c8e10d8f821") %>%
mutate(DateTime = as.POSIXct(DateTime, "%Y-%m-%d %H:%M:%S", tz = "EST"))
## Load in diffusive fluxes:
## Calculated using: 1b_Diffusive_Fluxes.R
diff_flux <- read_csv("https://pasta-s.lternet.edu/package/data/eml/edi/1102/4/dce28b69ccf1b2b138ad3c8e10d8f821") %>%
mutate(DateTime = as.POSIXct(DateTime, "%Y-%m-%d %H:%M:%S", tz = "EST"))
## Load in diffusive fluxes:
## Calculated using: 1b_Diffusive_Fluxes.R
diff_flux <- read_csv("https://pasta-s.lternet.edu/package/data/eml/edi/1102/4/dce28b69ccf1b2b138ad3c8e10d8f821")
View(diff_flux)
pacman::p_load("RCurl","tidyverse","lubridate", "plotly", "magrittr", "suncalc")
rm(list=ls()) #let's start with a blank slate
# Set working directory
wd <- getwd()
setwd(wd)
###2) Download the "raw" meteorological FCR datasets from GitHub and aggregate into 1 file:
#a. Past Met data, manual downloads
#download current met data from GitHub
download.file("https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-metstation-data/FCRmet.csv", paste0(wd, "/Data/FCRmet_2022.csv"))
#download maintenance file
download.file("https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-metstation-data/MET_MaintenanceLog.txt", paste0(wd, "/Data/FCR_Met_Maintenance_2015_2021.txt"))
#Run the function and create targets dataset with daily flux mean averaged over...
#..all the available half hourly values without any cut-off
#Use a predefined function to import the data
library(patchwork)
source("generate_EddyFLux_ghg_targets_function_without_cutoff.R")
install.packages("pacman")
targets_without_stable_mean <- generate_EddyFlux_ghg_targets_function(
flux_current_data_file <- "https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataNotYetUploadedToEDI/EddyFlux_Processing/EddyPro_Cleaned_L1.csv",
flux_edi_data_file <- "https://pasta-s.lternet.edu/package/data/eml/edi/692/11/e0976e7a6543fada4cbf5a1bb168713b",
met_current_data_file <- "https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-metstation-data-qaqc/FCRmet_L1.csv",
met_edi_data_file <- "https://pasta.lternet.edu/package/data/eml/edi/389/8/d4c74bbb3b86ea293e5c52136347fbb0")
library(tidyverse)
source("generate_EddyFLux_ghg_targets_function_without_cutoff.R")
targets_without_stable_mean <- generate_EddyFlux_ghg_targets_function(
flux_current_data_file <- "https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataNotYetUploadedToEDI/EddyFlux_Processing/EddyPro_Cleaned_L1.csv",
flux_edi_data_file <- "https://pasta-s.lternet.edu/package/data/eml/edi/692/11/e0976e7a6543fada4cbf5a1bb168713b",
met_current_data_file <- "https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-metstation-data-qaqc/FCRmet_L1.csv",
met_edi_data_file <- "https://pasta.lternet.edu/package/data/eml/edi/389/8/d4c74bbb3b86ea293e5c52136347fbb0")
library(hms)
source("generate_EddyFLux_ghg_targets_function_without_cutoff.R")
install.packages("pacman")
targets_without_stable_mean <- generate_EddyFlux_ghg_targets_function(
flux_current_data_file <- "https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataNotYetUploadedToEDI/EddyFlux_Processing/EddyPro_Cleaned_L1.csv",
flux_edi_data_file <- "https://pasta-s.lternet.edu/package/data/eml/edi/692/11/e0976e7a6543fada4cbf5a1bb168713b",
met_current_data_file <- "https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-metstation-data-qaqc/FCRmet_L1.csv",
met_edi_data_file <- "https://pasta.lternet.edu/package/data/eml/edi/389/8/d4c74bbb3b86ea293e5c52136347fbb0")
#Run the function and create targets dataset with daily flux mean averaged over...
#..all the available half hourly values without any cut-off
#Use a predefined function to import the data
library(patchwork)
library(hms)
source("generate_EddyFLux_ghg_targets_function_without_cutoff.R")
install.packages("pacman")
targets_without_stable_mean <- generate_EddyFlux_ghg_targets_function(
flux_current_data_file <- "https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataNotYetUploadedToEDI/EddyFlux_Processing/EddyPro_Cleaned_L1.csv",
flux_edi_data_file <- "https://pasta-s.lternet.edu/package/data/eml/edi/692/11/e0976e7a6543fada4cbf5a1bb168713b",
met_current_data_file <- "https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-metstation-data-qaqc/FCRmet_L1.csv",
met_edi_data_file <- "https://pasta.lternet.edu/package/data/eml/edi/389/8/d4c74bbb3b86ea293e5c52136347fbb0")
library(readr)
source("generate_EddyFLux_ghg_targets_function_without_cutoff.R")
targets_without_stable_mean <- generate_EddyFlux_ghg_targets_function(
flux_current_data_file <- "https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataNotYetUploadedToEDI/EddyFlux_Processing/EddyPro_Cleaned_L1.csv",
flux_edi_data_file <- "https://pasta-s.lternet.edu/package/data/eml/edi/692/11/e0976e7a6543fada4cbf5a1bb168713b",
met_current_data_file <- "https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-metstation-data-qaqc/FCRmet_L1.csv",
met_edi_data_file <- "https://pasta.lternet.edu/package/data/eml/edi/389/8/d4c74bbb3b86ea293e5c52136347fbb0")
#Create a csv file and save it!
write.csv(targets_without_stable_mean, "targets_without_stable_mean.csv")
#Plot the time series with the daily mean values
targets_without_stable_mean_wider <- targets_without_stable_mean %>%
pivot_wider(names_from = variable, values_from = observation)
p1 <- ggplot(targets_without_stable_mean_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = co2flux_umolm2s_mean), colour = "blue", alpha=0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CO2 daily mean") + xlab("") + ggtitle("U star filtering and cut-off not applied") +
ylim(-20, 20)
p2 <- ggplot(targets_without_stable_mean_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = ch4flux_umolm2s_mean), colour = "red", alpha = 0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CH4 daily mean") + xlab("") + ggtitle("U star filtering and cut-off not applied") +
ylim(-0.045, 0.052)
p1/p2
#Run the function and create targets dataset with daily flux mean averaged over...
#..all the available half hourly values without any cut-off
#Use a predefined function to import the data
library(patchwork)
p1/p2
#Now, let's run the function and create targets dataset with daily flux mean averaged over...
#..all the available half hourly values with the cut-off at 30!
#Use a predefined function to import the data
source("generate_EddyFLux_ghg_targets_function_with_cutoff.R")
targets_with_stable_mean <- generate_EddyFlux_ghg_targets_function(
flux_current_data_file <- "https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataNotYetUploadedToEDI/EddyFlux_Processing/EddyPro_Cleaned_L1.csv",
flux_edi_data_file <- "https://pasta-s.lternet.edu/package/data/eml/edi/692/11/e0976e7a6543fada4cbf5a1bb168713b",
met_current_data_file <- "https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-metstation-data-qaqc/FCRmet_L1.csv",
met_edi_data_file <- "https://pasta.lternet.edu/package/data/eml/edi/389/8/d4c74bbb3b86ea293e5c52136347fbb0")
#Create a csv file and save it!
write.csv(targets_with_stable_mean, "targets_with_stable_mean.csv")
#Plot the time series with the daily mean values
targets_with_stable_mean_wider <- targets_with_stable_mean %>%
pivot_wider(names_from = variable, values_from = observation)
p3 <- ggplot(targets_with_stable_mean_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = co2flux_umolm2s_mean), colour = "blue", alpha=0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CO2 daily mean") + xlab("") + ggtitle("No U star filtering but cut-off applied at 30") +
ylim(-20, 20)
p4 <- ggplot(targets_with_stable_mean_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = ch4flux_umolm2s_mean), colour = "red", alpha = 0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CH4 daily mean") + xlab("") + ggtitle("No U star filtering but cut-off applied at 30") +
ylim(-0.045, 0.052)
p1/p3
p2/p4
###2) Download the "raw" meteorological FCR datasets from GitHub and aggregate into 1 file:
#a. Past Met data, manual downloads
#download current met data from GitHub
download.file("https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-metstation-data/FCRmet.csv", paste0(wd, "/Data/FCRmet_2022.csv"))
targets_with_stable_mean <- generate_EddyFlux_ghg_targets_function(
flux_current_data_file <- "https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataNotYetUploadedToEDI/EddyFlux_Processing/EddyPro_Cleaned_L1.csv",
flux_edi_data_file <- "https://pasta-s.lternet.edu/package/data/eml/edi/692/11/e0976e7a6543fada4cbf5a1bb168713b",
met_current_data_file <- "https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-metstation-data-qaqc/FCRmet_L1.csv",
met_edi_data_file <- "https://pasta.lternet.edu/package/data/eml/edi/389/8/d4c74bbb3b86ea293e5c52136347fbb0")
#Create a csv file and save it!
write.csv(targets_with_stable_mean, "targets_with_stable_mean.csv")
#Plot the time series with the daily mean values
targets_with_stable_mean_wider <- targets_with_stable_mean %>%
pivot_wider(names_from = variable, values_from = observation)
p3 <- ggplot(targets_with_stable_mean_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = co2flux_umolm2s_mean), colour = "blue", alpha=0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CO2 daily mean") + xlab("") + ggtitle("No U star filtering but cut-off applied at 30") +
ylim(-20, 20)
p4 <- ggplot(targets_with_stable_mean_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = ch4flux_umolm2s_mean), colour = "red", alpha = 0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CH4 daily mean") + xlab("") + ggtitle("No U star filtering but cut-off applied at 30") +
ylim(-0.045, 0.052)
p1/p3
p2/p4
library(readr)
X2024_05_10_EC_processed_withBDScript <- read_csv("2024-05-10_EC_processed_withBDScript.csv")
View(X2024_05_10_EC_processed_withBDScript)
View(X2024_05_10_EC_processed_withBDScript)
View(targets_with_stable_mean)
View(targets_without_stable_mean_wider)
getwd()
#Load the dataset after wind directions and u star filtering with Brenda's code
u_star_applied <- read_csv("./2024-05-10_EC_processed_withBDScript.csv")
View(u_star_applied)
u_star_applied$date <- date(u_star_applied$DateTime)
#Count the number of half hourly values and apply the cut-off
flux_count <- u_star_applied %>%
select(DateTime, NEE_uStar_orig, ch4_flux_uStar_orig)%>%
dplyr::rename(co2flux_umolm2s_mean = NEE_uStar_orig,
ch4flux_umolm2s_mean = ch4_flux_uStar_orig) %>% # rename columns
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(frequency = n()) %>%#count hh values and filter >=20 only
filter(frequency >= 20) %>%
ungroup()
#Count the number of half hourly values and apply the cut-off
flux_count <- u_star_applied %>%
select(date, DateTime, NEE_uStar_orig, ch4_flux_uStar_orig)%>%
dplyr::rename(co2flux_umolm2s_mean = NEE_uStar_orig,
ch4flux_umolm2s_mean = ch4_flux_uStar_orig) %>% # rename columns
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(frequency = n()) %>%#count hh values and filter >=20 only
filter(frequency >= 20) %>%
ungroup()
View(flux_count)
#Count the number of half hourly values and apply the cut-off
flux_count <- u_star_applied %>%
select(date, DateTime, NEE_uStar_orig, ch4_flux_uStar_orig)%>%
dplyr::rename(co2flux_umolm2s_mean = NEE_uStar_orig,
ch4flux_umolm2s_mean = ch4_flux_uStar_orig) %>% # rename columns
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(frequency = n(), na.rm = TRUE) %>%#count hh values and filter >=20 only
filter(frequency >= 20) %>%
ungroup()
View(flux_count)
#Count the number of half hourly values and apply the cut-off
flux_count <- u_star_applied %>%
select(date, DateTime, NEE_uStar_orig, ch4_flux_uStar_orig)%>%
na.omit() %>%
dplyr::rename(co2flux_umolm2s_mean = NEE_uStar_orig,
ch4flux_umolm2s_mean = ch4_flux_uStar_orig) %>% # rename columns
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(frequency = n()) %>%#count hh values and filter >=20 only
filter(frequency >= 20) %>%
ungroup()
View(flux_count)
View(flux_count)
#Load the dataset after wind directions and u star filtering with Brenda's code
u_star_applied <- read_csv("./2024-05-10_EC_processed_withBDScript.csv")
#Extract the date column and add to the dataset
u_star_applied$date <- date(u_star_applied$DateTime)
#Remove the NAs separately for columns NEE and CH4
flux_count_NEE <- u_star_applied %>%
select(date, DateTime, NEE_uStar_orig)%>%
na.omit()
flux_count_CH4 <- u_star_applied %>%
select(date, DateTime, ch4_flux_uStar_orig)%>%
na.omit()
View(flux_count_NEE)
View(flux_count_CH4)
View(flux_count_NEE)
#Count the number of half hourly values and apply the cut-off
daily_NEE <- flux_count_NEE  %>%
select(date, DateTime, NEE_uStar_orig)%>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(frequency = n(),
co2flux_umolm2s_mean = mean(NEE_uStar_orig)) %>% #count hh values and filter >=20 only
#filter(frequency >= 20) %>%
ungroup()
View(daily_NEE)
#For CH4
daily_CH4 <- flux_count_CH4  %>%
select(date, DateTime, ch4_flux_uStar_orig) %>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(frequency = n(),
ch4flux_umolm2s_mean = mean(ch4_flux_uStar_orig)) %>% #count hh values and filter >=20 only
#filter(frequency >= 20) %>%
ungroup()
View(daily_CH4)
View(daily_CH4)
targets_df_ustar <- daily_NEE %>%
filter(frequency >= 20) %>%
#ungroup()%>%
drop_na(date)%>% # drop when we have timezone issues with daylight savings
mutate(datetime=(paste0(date," ","00:00:00")))%>%
#drop_na(datetime) %>%
mutate(Reservoir='fcre')%>% # change the name to the the reservoir code for FLARE
mutate(Depth_m = NA)%>%
select(-date, frequency)%>%
rename(site_id=Reservoir, # rename the columns for standard notation
depth=Depth_m)%>%
pivot_longer(cols=c(co2flux_umolm2s_mean), # make the wide data frame into a long one so each observation has a depth
names_to='variable',
values_to='observation')%>%
select(c('datetime', 'site_id', 'depth', "observation", 'variable')) # rearrange order of columns
View(targets_df_ustar)
215/1055
#Format as per FLARE
targets_df_ustarCO2 <- daily_NEE %>%
filter(frequency >= 20) %>%
#ungroup()%>%
drop_na(date)%>% # drop when we have timezone issues with daylight savings
mutate(datetime=(paste0(date," ","00:00:00")))%>%
#drop_na(datetime) %>%
mutate(Reservoir='fcre')%>% # change the name to the the reservoir code for FLARE
mutate(Depth_m = NA)%>%
select(-date, frequency)%>%
rename(site_id=Reservoir, # rename the columns for standard notation
depth=Depth_m)%>%
pivot_longer(cols=c(co2flux_umolm2s_mean), # make the wide data frame into a long one so each observation has a depth
names_to='variable',
values_to='observation')%>%
select(c('datetime', 'site_id', 'depth', "observation", 'variable')) # rearrange order of columns
#For CH4
daily_CH4 <- flux_count_CH4  %>%
select(date, DateTime, ch4_flux_uStar_orig) %>%
group_by(date)%>% # average if there are more than one sample taken during that day
summarise(frequency = n(),
ch4flux_umolm2s_mean = mean(ch4_flux_uStar_orig)) %>% #count hh values and filter >=20 only
#filter(frequency >= 20) %>%
ungroup()
#Format as per FLARE
targets_df_ustarCH4 <- daily_CH4 %>%
filter(frequency >= 20) %>%
drop_na(date)%>% # drop when we have timezone issues with daylight savings
mutate(datetime=(paste0(date," ","00:00:00")))%>%
mutate(Reservoir='fcre')%>% # change the name to the the reservoir code for FLARE
mutate(Depth_m = NA)%>%
select(-date, frequency)%>%
rename(site_id=Reservoir, # rename the columns for standard notation
depth=Depth_m)%>%
pivot_longer(cols=c(ch4flux_umolm2s_mean), # make the wide data frame into a long one so each observation has a depth
names_to='variable',
values_to='observation')%>%
select(c('datetime', 'site_id', 'depth', "observation", 'variable')) # rearrange order of columns
185/1121
View(targets_df_ustarCO2)
targets_df_ustarCO2_wider <- targets_df_ustarCO2 %>%
pivot_wider(names_from = variable, values_from = observation)
targets_df_ustarCH4_wider <- targets_df_ustarCH4 %>%
pivot_wider(names_from = variable, values_from = observation)
p1 <- ggplot(targets_df_ustarCO2_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = co2flux_umolm2s_mean), colour = "red", alpha = 0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CO2 daily mean") + xlab("") + ggtitle("Wind, U star and cut-off all applied") +
ylim(-20, 20)
p2 <- ggplot(targets_df_ustarCH4_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = ch4flux_umolm2s_mean), colour = "red", alpha = 0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CH4 daily mean") + xlab("") + ggtitle("Wind, U star and cut-off all applied") +
ylim(-0.045, 0.052)
#Plot the time series with the daily mean values
targets_without_stable_mean_wider <- targets_without_stable_mean %>%
pivot_wider(names_from = variable, values_from = observation)
p3 <- ggplot(targets_without_stable_mean_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = co2flux_umolm2s_mean), colour = "blue", alpha=0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CO2 daily mean") + xlab("") + ggtitle("U star filtering and cut-off not applied") +
ylim(-20, 20)
p4 <- ggplot(targets_without_stable_mean_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = ch4flux_umolm2s_mean), colour = "red", alpha = 0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CH4 daily mean") + xlab("") + ggtitle("U star filtering and cut-off not applied") +
ylim(-0.045, 0.052)
#Plot the time series with the daily mean values
targets_with_stable_mean_wider <- targets_with_stable_mean %>%
pivot_wider(names_from = variable, values_from = observation)
p5 <- ggplot(targets_with_stable_mean_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = co2flux_umolm2s_mean), colour = "blue", alpha=0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CO2 daily mean") + xlab("") + ggtitle("No U star filtering but cut-off applied at 20") +
ylim(-20, 20)
p6 <- ggplot(targets_with_stable_mean_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = ch4flux_umolm2s_mean), colour = "red", alpha = 0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CH4 daily mean") + xlab("") + ggtitle("No U star filtering but cut-off applied at 20") +
ylim(-0.045, 0.052)
p3/p5/p1
p1 <- ggplot(targets_df_ustarCO2_wider, aes(x = as.Date(datetime)))+
geom_point(aes(y = co2flux_umolm2s_mean), colour = "blue", alpha = 0.5) +
scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
ylab("CO2 daily mean") + xlab("") + ggtitle("Wind, U star and cut-off all applied") +
ylim(-20, 20)
p3/p5/p1
p4/p6/p2
## Clear workspace
rm(list = ls())
## Set working directory
wd <- getwd()
setwd(wd)
