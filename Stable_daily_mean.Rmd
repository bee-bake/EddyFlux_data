---
title: "Stable Half Hourly Mean"
author: "Bibek Kandel"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#How many half-hourly values are needed to get a stable daily mean as to use it in forecasting?

This script analyzes the mean and standard deviation (SD) calculated from randomly sampled half-hourly values from a complete dataset consisting of 48 values for a day. The random sampling is carried out 48 times for each run, thus, giving rise to 48 different means and SD. This process is reiterated 48 times to further randomize the process.

```{r}
#Clear workspace
rm(list=ls())
```

```{r}
#Load packages
library(tidyverse)
library(lubridate)
library(ggplot2)
```

```{r}
# Read compiled file: From Eddy Pro using basic processing
# Original file from Brenda on 11 May 2021
# Light data cleaning using EddyPro_CleanUp.R

dt1 <-read_csv("C:/Users/13188/Desktop/Reservoirs/Data/DataNotYetUploadedToEDI/EddyFlux_Processing/EddyPro_Cleaned_L1.csv")

```

```{r}
# Check here on EDI an make sure you are using the most up to date file.
# https://portal.edirepository.org/nis/mapbrowse?packageid=edi.1061.2 
# if not pulll in the file path from EDI


# read in the data file downloaded from EDI
dt2 <-read_csv("https://pasta.lternet.edu/package/data/eml/edi/1061/3/e0976e7a6543fada4cbf5a1bb168713b") 

#Create a full dataset
ec_full <- bind_rows(dt2, dt1)
```

```{r}
#Extract the mean flux values 
hh_flux <-  ec_full %>%
  select(c('date', 'time','co2_flux_umolm2s','ch4_flux_umolm2s'))

#Count how many data points are available at each half hour interval
availability <- hh_flux %>% group_by(time) %>% 
  select(co2_flux_umolm2s, ch4_flux_umolm2s) %>% 
  summarise(co2_available = 100-sum(is.na(co2_flux_umolm2s))/n()*100,
            ch4_available = 100-sum(is.na(ch4_flux_umolm2s))/n()*100,
            co2_n = n() - sum(is.na(co2_flux_umolm2s)),
            ch4_n = n() - sum(is.na(ch4_flux_umolm2s)))

availability

```

```{r}
#Plot the data availability by half-hour
avail <- availability %>%
  pivot_longer(cols = co2_available:ch4_available, names_to = "variable", 
               values_to = "flux_value")

ggplot()+
  geom_point(data =avail, aes(x=time, y=flux_value, color = variable))+
  ylab("% of available half-hourly data")

```

```{r}
#calculate the mean flux values by half hour
hh_flux2 = subset(hh_flux, select = -c(date))
hh_mean <- hh_flux2 %>%
  group_by(time) %>%
  summarise_all(mean, na.rm = TRUE)

#Plot the values
ggplot()+
  geom_point(data =hh_mean, aes(x=time, y=co2_flux_umolm2s))+
  ylab("CO2")+
  ggtitle("Half hourly mean values across the day")

ggplot()+
  geom_point(data =hh_mean, aes(x=time, y=ch4_flux_umolm2s))+
  ylab("CH4")+
  ggtitle("Half hourly mean values across the day")

```

```{r}
#Count the number of half-hourly values each day
hh_flux_noNA <- na.omit(hh_flux)
hh_count <- hh_flux_noNA %>%
  group_by(date) %>%
  summarise(frequency = n())

ggplot(hh_count, aes(x = frequency)) +
  ylab("No of days")+
  xlab("Half hours")+
  geom_bar()

```

```{r}
#Calculate the mean flux values by day
hh_dmean <- hh_flux_noNA %>%
  select(c(date, co2_flux_umolm2s,ch4_flux_umolm2s)) %>%
  group_by(date) %>%
  summarise_all(list(mean, sd)) %>%
  rename(
    co2_mean = co2_flux_umolm2s_fn1,
    co2_sd = co2_flux_umolm2s_fn2,
    ch4_mean = ch4_flux_umolm2s_fn1,
    ch4_sd = ch4_flux_umolm2s_fn2,
    )

```

```{r}
#Join two dataframes with mean values and count values
hh_merged <- merge(x = hh_dmean, y = hh_count, by = "date", all = TRUE)

#Plot the mean values versus the number of half hourly values present
ggplot()+
  geom_point(data = hh_merged, aes(x=frequency, y=co2_mean, color = frequency))+
  xlab("Half hours")+
  ylab("mean_CO2")

ggplot()+
  geom_point(data = hh_merged, aes(x=frequency, y=ch4_mean, color = frequency))+
  xlab("Half hours")+
  ylab("mean_CH4")

```

```{r}
#Select the day with 48 half hourly values
hh_merged2 <- hh_merged %>% filter(frequency == 48)

```

```{r}
#Create a table with days having all 48 half hourly values
all_48 <- data.frame()

for(i in (hh_merged2$date)){
  curr_ens <- hh_flux_noNA |> 
    filter(date == i) |> 
    select(date, time, co2_flux_umolm2s, ch4_flux_umolm2s)
  
  all_48 <- bind_rows(all_48, curr_ens)
}

```

#If you want to run the analysis using 48 half-hourly values for a single day then use the code in the chunk below.

```{r}
#Or select a single day with all 48 half hourly values
oneday_48 <- hh_flux_noNA %>% filter(date == "2024-02-05")

```

#For mean

```{r}
#Random sampling using different sizes
all_runs_1day <- NULL
n = 1:48
for(j in 1:30) { #change j to change the no of ensemble members (repeats of run)
  multiple_runs <- tibble(run_no = rep(j, times = 48), s_size = n, samp_mean = as.double(NA))
  
  sampling_table <- tibble(run_no = rep(j, times = 48), s_size = n, samp_mean = as.double(NA))
  
  #random sampling of half hourly values anywhere between 1-48 and calculates mean
  for(i in 1:length(n)) {
    size = i
    rm <- sample(oneday_48$co2_flux_umolm2s, size = i, replace = FALSE) #Edit this for one-day run
    m_value <- mean(rm)
    df <- tibble(s_size = i, samp_mean = m_value)
    sampling_table <- sampling_table %>%
      rows_update(df, by = c("s_size"))
  }
  
  multiple_runs <- multiple_runs %>%
    rows_update(sampling_table, by = c("run_no", "s_size"))
  
  
  all_runs_1day <- dplyr::bind_rows(all_runs_1day, multiple_runs)
}

#SD of means grouped by sample size
all_runs_1day |> 
  group_by(s_size) |> 
  summarize(sd = sd(samp_mean)) |> 
  ggplot(aes(x=s_size, y=sd)) + 
  geom_line()+
  ggtitle("Distribution of SDs calculated from hh means - single day
  (Means are obtained from the means of randomly sampled hh values)")

```


#Now, let's use all the available days and half-hourly values for the analysis. The random sampling for each day is carried out for 5 different runs and the obtained means are averaged to get a single mean for that sample size and day.

#For mean and SD

```{r, warning=FALSE}
#Random sampling using different sizes
all_runs <- NULL
n = 1:48
#Extract the dates and check the range
md <- unique(as.numeric(format(all_48$date, "%Y%m%d")))
md <- ymd(unique(as.numeric(md)))
range(as.Date(md))

for (k in 1:length(md)) {
  
  for (l in 1:30) { #30 for the statistical threshold of minimum runs
    
    #change j to change the no of ensemble members (repeats of run)
    multiple_runs <- tibble(date = rep(md[k], times = 48), run_no = rep(l, times = 48), s_size = n, samp_mean = as.double(NA))
    
    sampling_table <- tibble(date = rep(md[k], times = 48), run_no = rep(l, times = 48), s_size = n, samp_mean = as.double(NA))
    
    doi <- md[k]
    sampling_dataset <- all_48 |> filter(date == doi)
    
    #random sampling of half hourly values anywhere between 1-48 and calculates mean
    for(j in 1:length(n)) {
      size = j
      rm <- sample(sampling_dataset$co2_flux_umolm2s, size = j, replace = FALSE) #Edit this for one-day run
      m_value <- mean(rm)
      df <- tibble(s_size = j, samp_mean = m_value)
      sampling_table <- sampling_table %>%
        rows_update(df, by = c("s_size"))
    }
    
    multiple_runs <- multiple_runs %>%
      rows_update(sampling_table, by = c("date", "run_no", "s_size"))
    
    all_runs <- dplyr::bind_rows(all_runs, multiple_runs)
  }
}

```

```{r, warning=FALSE}
#SD of means grouped by sample size
all_runs |> 
  group_by(s_size, date) |> 
  summarize(run_mean = sd(samp_mean)) |> 
  summarize(sd = mean(run_mean)) |> 
  ggplot(aes(x=s_size, y=sd, colour = "orange")) +
  geom_line(show.legend = F)+
  ggtitle("Distribution of SDs calculated from hh means - all days
  (Means are obtained from the means of multiple runs of randomly 
  sampled hh values)")

```

Questions:
1. How many times do we want to run the cycle (in this case 30 times) to         optimize the stability of the mean and sd?
   Ans: Typically, 30 runs are considered a threshold for the stable mean and         sd. The more number of runs, the more stable mean.
   
2. Does it matter if we sample from one big pool of all half hourly values or    just from the means of the respective day?
   Ans: We will sample from the half hourly values from the day of interest           and not from a common pool. 

3. How do we determine the cut-off for the stable mean and sd?

The goal is to find out at what point the graph stabilize, which to me seemingly happens between half-hours 20-40 (before it starts to increase between half hours 0-20). Determining "stability" is obviously subjective, but there must be some numerical approaches that can do so using some subjective threshold. I have looked at 4 different approaches, all of which have their weaknesses.

```{r, warning=FALSE}
#For single day
one_day_summarized <- all_runs_1day |> 
  group_by(s_size) |> 
  summarize(sd = sd(samp_mean))

#For all days
all_runs_summarized <- all_runs |> 
  group_by(s_size, date) |> 
  summarize(run_mean = sd(samp_mean)) |> 
  summarize(sd = mean(run_mean))

```

#1: Derivative of polynomial interpolation

My first approach was to perform polynomial interpolation and use its derivative to determine stabilization. For example, when the derivative value is below 0.5 for the last 30 time periods (alternatively the past 15 and next 15) we can say that the value has stabilized.

```{r}
library(polynom)

x_p <- unlist(one_day_summarized$s_size)
f_p <- unlist(one_day_summarized$sd)

#fitting third order polynomial regression
poly_eq <- lm(f_p ~ poly(x_p, 3, raw=TRUE))
summary(poly_eq)
coeff <- coef(poly_eq)

#Plot the function
f <- function(x_p) {
  return(coeff[1] + coeff[2]*x_p + coeff[3]*x_p^2 + coeff[4]*x_p^3)
}
dat <- tibble(x_p, f_p)
ggplot(dat, aes(x=x_p, y=f_p)) + 
  geom_point(size=1, col='blue') + 
  stat_function(fun = f, linewidth=1, alpha=0.4)
```


```{r, warning=FALSE}
x <- x_p[x_p < 50]

#Form an equation
a <- coeff[1]
b <- coeff[2]
c <- coeff[3]
d <- coeff[4]

subst <- function(e, ...) do.call(substitute, list(e[[1]], list(...)))
exp <- expression(a + b * x + c * x^2 + d * x^3)

#find the second order derivative
d1 <- D((exp), "x")
d2 <- D(d1, "x")

#find y-values
for (i in length(x)) {
  y = ((c * 2 + d * (3 * (2 * x))))
}

#Create new dataframe and plot
df <- tibble(x,y)
ggplot(df, aes(x = x, y = y)) +
       geom_line()
## we want the function to be linear and hence more stable!
```

Weaknesses:

(i) It is difficult to determine what degree of polynomial to use that works for all similar graphs

(ii) The derivative will have different values for a similar slope depending on the value range of the data

#2: Derivative of cubic spline interpolation

Similar to polynomial interpolation, cubic spline interpolation attempts to estimate a function corresponding to the data points. It makes for a better estimation (at least in this case), but using its derivative comes with some problems. The idea is the same as for polynomial interpolation: When the derivative is below a certain threshold over a number of periods, we can say the graph has stabilized.

Weaknesses:

(i) The function estimates the graph too well, giving high derivative values for relatively small value changes.

```{r}

## and one from Fritsch and Carlson (1980), Dougherty et al (1989)
x <- unlist(one_day_summarized$s_size)
f <- unlist(one_day_summarized$sd)

#run this chunk together
s0 <- splinefun(x, f)
s1 <- splinefun(x, f, method = "natural")
plot(x, f)
curve(s0(x), add = TRUE, col = 2, n = 1001) -> m0
curve(s1(x), add = TRUE, col = 3, n = 1001)
legend("right",
       paste0("splinefun( \"", c("fmm", "natural"), "\" )"),
       col = 2:4, lty = 1, bty = "n")

```
```{r}
## they seem identical, but are not quite:
xx <- m0$x
plot(xx, s0(xx) - s1(xx), type = "l",  col = 2, lwd = 2,
     main = "Difference fmm - natural"); abline(h = 0, lty = 3)

```
```{r, warning=FALSE}
#function: sd = a*hh^3 + b*hh^2 + c*hh + d
x <- xx[xx < 49] ## change this to determine cut-off value
ccol <- adjustcolor(2:4, 0.8)
matplot(x, cbind(s0(x, deriv = 2), s1(x, deriv = 2))^2,
        ylim = c(0,10), xaxt = "n",
        lwd = 2, col = ccol, type = "l", ylab = quote({{f*second}(x)}^2),
        main = expression({{f*second}(x)}^2 ~" for the two 'splines'"))
legend("topright",
       paste0("splinefun( \"", c("fmm", "natural"), "\" )"),
       lwd = 2, col  =  ccol, lty = 1:3, bty = "n")
abline(h = 0.5, v = 23, col = "black", lty = 2, lwd = 2)
axis(side = 1, at = c(5,10,15,20,25,30,35,40,45, 50))
## we want the function to be linear and hence more stable!
```
#3 Use the standard deviation over a moving window

The third approach is to calculate the standard deviation (STD) for n periods prior (5 is used in this example) and determine some threshold where if the STD goes below that threshold, we can say the value has stabilized. The value is normalized so that the approach can be generalized to all similar sets of data points, regardless of its value range.

```{r, warning=FALSE}
library(zoo)
library(gridExtra)
library(caret)
data <- f_p
moving_std_dev1 <- rollapply(data, width = 5, FUN = sd, fill = 0, align = "left")
#normalize data
process <- preProcess(as.data.frame(moving_std_dev1), method=c("range"))
scale_sd1 <- predict(process, as.data.frame(moving_std_dev1))

#alternatively
lag_apply <- function(f_p, n, callback){
  k = length(f_p);
  result = rep(0, k);
  for(i in 1 : (k - n + 1)){
    result[i] <- callback(f_p[i :  (i + n -1)]);
  }    
  return(result);
}

lag_apply(f_p, 5, function(f_p){sd(f_p)}) -> moving_std_dev2
#normalize data
process <- preProcess(as.data.frame(moving_std_dev2), method=c("range"))
scale_sd2 <- predict(process, as.data.frame(moving_std_dev2))

#plot
df1 <- tibble(x_p, scale_sd1)
df2 <- tibble(x_p, scale_sd2)
plot1 <- ggplot(df1, aes(x=as.numeric(unlist(x_p)), y=as.numeric(unlist(scale_sd1)))) +
  geom_line() + ylab("moving sd") + xlab("half hours")
plot2 <- ggplot(df2, aes(x=as.numeric(unlist(x_p)), y=as.numeric(unlist(scale_sd2)))) +  geom_line() + ylab("moving sd") + xlab("half hours")

grid.arrange(plot1, plot2, ncol = 2)
```

Weaknesses:

(i) The STD can have low values before stabilization occurs. Possibly combat this by adding a condition that the STD must be below that threshold for a certain number of periods? Additionally, how many half hours for a period?

#4 Use the relative standard deviation over a moving window

The last approach is to use the relative standard deviation, calculated for n periods prior (5 in this case). It is calculated by first calculating the STD as in approach 3, then multiply it by 100 and divide it by the mean value of those last n periods. What you get is a percentage representation of the standard deviation over the last n periods.

```{r, warning=FALSE}
lag_apply <- function(f_p, n, callback){
  k = length(f_p);
  result = rep(0, k);
  for(i in 1 : (k - n + 1)){
    result[i] <- callback(f_p[i :  (i + n -1)]);
  }    
  return(result);
}
lag_apply(f_p, 5, function(f_p){mean(f_p)}) -> moving_mean
lag_apply(f_p, 5, function(f_p){((sd(f_p)*100)/moving_mean)}) -> moving_std_dev2

#plot
df2 <- tibble(x_p, moving_std_dev2)
ggplot(df2, aes(x=as.numeric(unlist(x_p)), y=as.numeric(unlist(moving_std_dev2)))) +  geom_line() + ylab("percentage representation of the standard deviation") + xlab("half hours")

```

Weaknesses:

(i) Similar to approach 3, the relative standard deviation can have low values before stabilization occurs.

Resources:
1. https://www.reddit.com/r/math/comments/qxipra/determine_when_a_value_has_stabilized_my/

2. https://stat.ethz.ch/R-manual/R-patched/library/stats/html/splinefun.html

3. https://cran.r-project.org/web/packages/interp/vignettes/partDeriv.pdf

4. https://www.math.colostate.edu/~shriner/sec-1-3-derivative-pt.html


#Here, we are using polynomial regression model fit and it's second derivative to inform the point of inflection and hence, stable sd. This will run through all the days with complete 48 hh values and build a distribution of the cut-off hh values obtained from all the days.

```{r}
#Extract the dates and check the range
md <- unique(as.numeric(format(all_48$date, "%Y%m%d")))
md <- ymd(unique(as.numeric(md)))
#check min and max dates
md[c(which.min(md), which.max(md))]
cutoff_dist <- NULL

for (k in 1:length(md)) {

#select a single day with all 48 half hourly values
doi <- md[k]
oneday_48 <- all_48 %>% filter(date == doi)

#Random sampling using different sizes
all_runs_1day <- NULL
n = 1:48
for(j in 1:30) { #change j to change the no of ensemble members (repeats of run)
  multiple_runs <- tibble(run_no = rep(j, times = 48), s_size = n, samp_mean = as.double(NA))
  
  sampling_table <- tibble(run_no = rep(j, times = 48), s_size = n, samp_mean = as.double(NA))
  
  #random sampling of half hourly values anywhere between 1-48 and calculates mean
  for(i in 1:length(n)) {
    size = i
    rm <- sample(oneday_48$co2_flux_umolm2s, size = i, replace = FALSE) #Edit this for one-day run
    m_value <- mean(rm)
    df <- tibble(s_size = i, samp_mean = m_value)
    sampling_table <- sampling_table %>%
      rows_update(df, by = c("s_size"))
  }
  
  multiple_runs <- multiple_runs %>%
    rows_update(sampling_table, by = c("run_no", "s_size"))
  
  
  all_runs_1day <- dplyr::bind_rows(all_runs_1day, multiple_runs)
}

#For single day
one_day_summarized <- all_runs_1day |> 
  group_by(s_size) |> 
  summarize(sd = sd(samp_mean))


x_p <- unlist(one_day_summarized$s_size)
f_p <- unlist(one_day_summarized$sd)

#fitting third order polynomial regression
poly_eq <- lm(f_p ~ poly(x_p, 3, raw=TRUE))
summary(poly_eq)
coeff <- coef(poly_eq)

#Plot the function
f <- function(x_p) {
  return(coeff[1] + coeff[2]*x_p + coeff[3]*x_p^2 + coeff[4]*x_p^3)
}

x <- x_p[x_p < 50]

#Extract coeffs to form an equation
a <- coeff[1]
b <- coeff[2]
c <- coeff[3]
d <- coeff[4]

subst <- function(e, ...) do.call(substitute, list(e[[1]], list(...)))
exp <- expression(a + b * x + c * x^2 + d * x^3)

#find the second order derivative
d1 <- D((exp), "x")
d2 <- D(d1, "x")

#find y-values
for (l in length(x)) {
  y = ((c * 2 + d * (3 * (2 * x))))
}

#Use a function to find the minimum positive slope
minpositive = function(x) min(x[x > 0])
min_deriv <- minpositive(y)

#Create new dataframe and plot
df <- tibble(x,y)
df_new <- df[df$y == min_deriv, ]
df_new <- df_new |> 
  mutate(date = doi)

cutoff_dist <- dplyr::bind_rows(cutoff_dist, df_new)

}

```

#Let's plot the distribution of the cut-offs and check the most likely value.

```{r}
#Count the number and plot the graph
cut_off_count <- cutoff_dist %>%
  group_by(x) %>%
  summarise(frequency = n())

ggplot(aes(x = x, y = frequency), data = cut_off_count) +
  ylab("No of days with cut-off at zero")+
  xlab("Half hours")+
  geom_bar(stat = 'identity')

```

#Now, we see from the plot that the most likely cut-off is at 29 half hourly values. However, one important consideration here is how much data is lost after applying this cut-off. Hence, we import the dataset filtered for wind directions and u star threshold and answer this question.

```{r, warning=FALSE}
#Import the data filtered for wind directions only
gap_unfilled <- read_csv("Eddy_fcr_footprint_full.csv")

#Split date and time
#gap_unfilled$Date <- as.Date(gap_filled$DateTime)
#gap_unfilled$Time <- format(as.POSIXct(gap_filled$DateTime), format = "%H:%M:%S") 

#Count the number of half-hourly values each day
gap_unfilled_countCO2 <- gap_unfilled %>%
  select(date, NEE.med) %>%
  na.omit() %>%
  group_by(date) %>%
  summarise(frequency = n())

p1 <- ggplot(gap_unfilled_countCO2, aes(x = frequency)) +
  ylab("No of days")+
  xlab("Half hours")+ ggtitle("Data availability after footprint filter for CO2") +
  geom_bar()

gap_unfilled_countCH4 <- gap_unfilled %>%
  select(date, ch4.med) %>%
  na.omit() %>%
  group_by(date) %>%
  summarise(frequency = n())

p2 <- ggplot(gap_unfilled_countCH4, aes(x = frequency)) +
  ylab("No of days")+
  xlab("Half hours")+ ggtitle("Data availability after footprint filter for CH4") +
  geom_bar()

```


```{r, warning=FALSE}
#Import the gap filled dataset but use columns that are not gap filled
gap_filled <- read_csv("2024-05-10_EC_processed_withBDScript.csv")

#Split date and time
gap_filled$Date <- as.Date(gap_filled$DateTime)
gap_filled$Time <- format(as.POSIXct(gap_filled$DateTime), format = "%H:%M:%S") 

#Count the number of half-hourly values each day
gap_filled_countCO2 <- gap_filled %>%
  select(Date, NEE_uStar_orig) %>%
  na.omit() %>%
  group_by(Date) %>%
  summarise(frequency = n())

p3 <- ggplot(gap_filled_countCO2, aes(x = frequency)) +
  ylab("No of days")+
  xlab("Half hours")+ ggtitle("Data availability after footprint and u star filter for CO2") +
  geom_bar()

gap_filled_countCH4 <- gap_filled %>%
  select(Date, ch4_flux_uStar_orig) %>%
  na.omit() %>%
  group_by(Date) %>%
  summarise(frequency = n())

p4 <- ggplot(gap_filled_countCH4, aes(x = frequency)) +
  ylab("No of days")+
  xlab("Half hours")+ ggtitle("Data availability after footprint and u star filter for CH4") +
  geom_bar()

p1/p3
p2/p4
```

#Let's apply the cut-off at 30 half hourly values and assess the data loss after filtering for wind directions only.

```{r}
#Calculate the proportion of data represented by available half hourly values
data_loss_CO2 <- gap_unfilled_countCO2 %>%
  group_by(frequency) %>%
  summarise(no_days = n()) %>%
  mutate(proportion = (no_days/(sum(no_days))))

#Since some frequency values are missing, create a tibble with complete series
z = 1:48
complete_hh_blank <- tibble(frequency = rep(z, times = 1))
complete_hh_series <- merge(complete_hh_blank, data_loss_CO2, by = "frequency", all = TRUE)
complete_hh_series[is.na(complete_hh_series)] <- 0
complete_hh_series$cum_proportion[1] <- complete_hh_series$proportion[1]

#Calculate cumulative proportion of available data represented by each half hourly value
for (i in 2:length(complete_hh_series$frequency)) {
  complete_hh_series$cum_proportion[i] = (complete_hh_series$cum_proportion[i-1] + complete_hh_series$proportion[i])

}

#Plot the line graph
ggplot(complete_hh_series, aes(x = frequency, y = cum_proportion)) +
  ylab("Proportion")+
  xlab("Half hours")+ ggtitle("Data availability after footprint filter for CO2 with cut-off at 30") +
  geom_line() + geom_vline(xintercept=30, col = "red", lty = 2, lwd = 0.7) +
  geom_hline(yintercept=0.92, col = "red", lty = 2, lwd = 0.7)

```
#Let's apply the cut-off at 30 half hourly values and assess the data loss after filtering for both wind directions and u star threshold.

```{r}
#Calculate the proportion of data represented by available half hourly values
data_loss_CO2 <- gap_filled_countCO2 %>%
  group_by(frequency) %>%
  summarise(no_days = n()) %>%
  mutate(proportion = (no_days/(sum(no_days))))

#Since some frequency values are missing, create a tibble with complete series
z = 1:48
complete_hh_blank <- tibble(frequency = rep(z, times = 1))
complete_hh_series <- merge(complete_hh_blank, data_loss_CO2, by = "frequency", all = TRUE)
complete_hh_series[is.na(complete_hh_series)] <- 0
complete_hh_series$cum_proportion[1] <- complete_hh_series$proportion[1]

#Calculate cumulative proportion of available data represented by each half hourly value
for (i in 2:length(complete_hh_series$frequency)) {
  complete_hh_series$cum_proportion[i] = (complete_hh_series$cum_proportion[i-1] + complete_hh_series$proportion[i])

}

#Plot the line graph
ggplot(complete_hh_series, aes(x = frequency, y = cum_proportion)) +
  ylab("Proportion")+
  xlab("Half hours")+ ggtitle("Data availability after footprint and u star filter for CO2 with cut-off at 30") +
  geom_line() + geom_vline(xintercept=30, col = "red", lty = 2, lwd = 0.7) +
  geom_hline(yintercept=0.96, col = "red", lty = 2, lwd = 0.7)

```
```








